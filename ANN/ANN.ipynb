{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "CVXDCsZE1Na4",
        "outputId": "bdd1a4d1-3541-4bbf-e4f4-45b870a6e803"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 27ms/step - accuracy: 0.5262 - loss: 1.3429 - val_accuracy: 0.7125 - val_loss: 1.0271\n",
            "Epoch 2/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6511 - loss: 1.0831 - val_accuracy: 0.7750 - val_loss: 0.9083\n",
            "Epoch 3/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7194 - loss: 0.9593 - val_accuracy: 0.8250 - val_loss: 0.8471\n",
            "Epoch 4/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8097 - loss: 0.8959 - val_accuracy: 0.8625 - val_loss: 0.8035\n",
            "Epoch 5/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8212 - loss: 0.8637 - val_accuracy: 0.8813 - val_loss: 0.7745\n",
            "Epoch 6/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8344 - loss: 0.8083 - val_accuracy: 0.9000 - val_loss: 0.7494\n",
            "Epoch 7/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8259 - loss: 0.8208 - val_accuracy: 0.9000 - val_loss: 0.7308\n",
            "Epoch 8/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8425 - loss: 0.8056 - val_accuracy: 0.9125 - val_loss: 0.7172\n",
            "Epoch 9/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8479 - loss: 0.7816 - val_accuracy: 0.9250 - val_loss: 0.6992\n",
            "Epoch 10/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8564 - loss: 0.7564 - val_accuracy: 0.9187 - val_loss: 0.6937\n",
            "Epoch 11/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8915 - loss: 0.6988 - val_accuracy: 0.9125 - val_loss: 0.6840\n",
            "Epoch 12/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8592 - loss: 0.7300 - val_accuracy: 0.9312 - val_loss: 0.6746\n",
            "Epoch 13/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8688 - loss: 0.7122 - val_accuracy: 0.9375 - val_loss: 0.6716\n",
            "Epoch 14/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9003 - loss: 0.6738 - val_accuracy: 0.9250 - val_loss: 0.6639\n",
            "Epoch 15/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8936 - loss: 0.7036 - val_accuracy: 0.9187 - val_loss: 0.6521\n",
            "Epoch 16/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9115 - loss: 0.6434 - val_accuracy: 0.9250 - val_loss: 0.6402\n",
            "Epoch 17/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9055 - loss: 0.6831 - val_accuracy: 0.9312 - val_loss: 0.6278\n",
            "Epoch 18/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9082 - loss: 0.6319 - val_accuracy: 0.9375 - val_loss: 0.6149\n",
            "Epoch 19/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9112 - loss: 0.6067 - val_accuracy: 0.9375 - val_loss: 0.6107\n",
            "Epoch 20/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8866 - loss: 0.6510 - val_accuracy: 0.9438 - val_loss: 0.5980\n",
            "Epoch 21/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9151 - loss: 0.6225 - val_accuracy: 0.9312 - val_loss: 0.6017\n",
            "Epoch 22/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9040 - loss: 0.5902 - val_accuracy: 0.9312 - val_loss: 0.5953\n",
            "Epoch 23/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9479 - loss: 0.5395 - val_accuracy: 0.9312 - val_loss: 0.5835\n",
            "Epoch 24/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9358 - loss: 0.5779 - val_accuracy: 0.9250 - val_loss: 0.5821\n",
            "Epoch 25/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9423 - loss: 0.5388 - val_accuracy: 0.9125 - val_loss: 0.5909\n",
            "Epoch 26/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9280 - loss: 0.5616 - val_accuracy: 0.9250 - val_loss: 0.5814\n",
            "Epoch 27/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9294 - loss: 0.5396 - val_accuracy: 0.9250 - val_loss: 0.5688\n",
            "Epoch 28/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9375 - loss: 0.5191 - val_accuracy: 0.9250 - val_loss: 0.5649\n",
            "Epoch 29/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9368 - loss: 0.5224 - val_accuracy: 0.9250 - val_loss: 0.5545\n",
            "Epoch 30/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8987 - loss: 0.5809 - val_accuracy: 0.9187 - val_loss: 0.5535\n",
            "Epoch 31/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9324 - loss: 0.5460 - val_accuracy: 0.9312 - val_loss: 0.5514\n",
            "Epoch 32/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9190 - loss: 0.5303 - val_accuracy: 0.9250 - val_loss: 0.5416\n",
            "Epoch 33/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9381 - loss: 0.5239 - val_accuracy: 0.9375 - val_loss: 0.5331\n",
            "Epoch 34/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9530 - loss: 0.4741 - val_accuracy: 0.9438 - val_loss: 0.5227\n",
            "Epoch 35/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9377 - loss: 0.4969 - val_accuracy: 0.9500 - val_loss: 0.5168\n",
            "Epoch 36/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9608 - loss: 0.4636 - val_accuracy: 0.9375 - val_loss: 0.5135\n",
            "Epoch 37/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9546 - loss: 0.4499 - val_accuracy: 0.9375 - val_loss: 0.5151\n",
            "Epoch 38/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9400 - loss: 0.4761 - val_accuracy: 0.9312 - val_loss: 0.5109\n",
            "Epoch 39/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9559 - loss: 0.4587 - val_accuracy: 0.9438 - val_loss: 0.5029\n",
            "Epoch 40/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9522 - loss: 0.4668 - val_accuracy: 0.9250 - val_loss: 0.5123\n",
            "Epoch 41/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9303 - loss: 0.5008 - val_accuracy: 0.9250 - val_loss: 0.5174\n",
            "Epoch 42/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9464 - loss: 0.4667 - val_accuracy: 0.9312 - val_loss: 0.5051\n",
            "Epoch 43/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9367 - loss: 0.4779 - val_accuracy: 0.9312 - val_loss: 0.4945\n",
            "Epoch 44/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9583 - loss: 0.4470 - val_accuracy: 0.9312 - val_loss: 0.4896\n",
            "Epoch 45/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9703 - loss: 0.4427 - val_accuracy: 0.9375 - val_loss: 0.4839\n",
            "Epoch 46/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9421 - loss: 0.4653 - val_accuracy: 0.9500 - val_loss: 0.4718\n",
            "Epoch 47/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9674 - loss: 0.4044 - val_accuracy: 0.9500 - val_loss: 0.4711\n",
            "Epoch 48/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9608 - loss: 0.4108 - val_accuracy: 0.9563 - val_loss: 0.4664\n",
            "Epoch 49/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9585 - loss: 0.4044 - val_accuracy: 0.9563 - val_loss: 0.4581\n",
            "Epoch 50/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - accuracy: 0.9638 - loss: 0.4055 - val_accuracy: 0.9500 - val_loss: 0.4519\n",
            "Epoch 51/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9523 - loss: 0.4086 - val_accuracy: 0.9438 - val_loss: 0.4557\n",
            "Epoch 52/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.9497 - loss: 0.4359 - val_accuracy: 0.9438 - val_loss: 0.4609\n",
            "Epoch 53/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.9542 - loss: 0.4293 - val_accuracy: 0.9563 - val_loss: 0.4533\n",
            "Epoch 54/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - accuracy: 0.9541 - loss: 0.4048 - val_accuracy: 0.9563 - val_loss: 0.4480\n",
            "Epoch 55/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 40ms/step - accuracy: 0.9561 - loss: 0.4083 - val_accuracy: 0.9500 - val_loss: 0.4505\n",
            "Epoch 56/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 53ms/step - accuracy: 0.9599 - loss: 0.3719 - val_accuracy: 0.9438 - val_loss: 0.4495\n",
            "Epoch 57/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - accuracy: 0.9767 - loss: 0.3763 - val_accuracy: 0.9500 - val_loss: 0.4473\n",
            "Epoch 58/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9634 - loss: 0.3614 - val_accuracy: 0.9500 - val_loss: 0.4483\n",
            "Epoch 59/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9316 - loss: 0.4409 - val_accuracy: 0.9500 - val_loss: 0.4436\n",
            "Epoch 60/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9687 - loss: 0.3716 - val_accuracy: 0.9563 - val_loss: 0.4208\n",
            "Epoch 61/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9681 - loss: 0.3539 - val_accuracy: 0.9563 - val_loss: 0.4252\n",
            "Epoch 62/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9708 - loss: 0.3705 - val_accuracy: 0.9500 - val_loss: 0.4316\n",
            "Epoch 63/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9847 - loss: 0.3247 - val_accuracy: 0.9438 - val_loss: 0.4214\n",
            "Epoch 64/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9719 - loss: 0.3427 - val_accuracy: 0.9375 - val_loss: 0.4155\n",
            "Epoch 65/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9701 - loss: 0.3475 - val_accuracy: 0.9438 - val_loss: 0.4246\n",
            "Epoch 66/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9755 - loss: 0.3371 - val_accuracy: 0.9500 - val_loss: 0.4283\n",
            "Epoch 67/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.9751 - loss: 0.3286 - val_accuracy: 0.9500 - val_loss: 0.4276\n",
            "Epoch 68/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.9779 - loss: 0.3299 - val_accuracy: 0.9500 - val_loss: 0.4297\n",
            "Epoch 69/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9289 - loss: 0.3953 - val_accuracy: 0.9500 - val_loss: 0.4143\n",
            "Epoch 70/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9616 - loss: 0.3296 - val_accuracy: 0.9500 - val_loss: 0.4054\n",
            "Epoch 71/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9735 - loss: 0.3154 - val_accuracy: 0.9438 - val_loss: 0.4053\n",
            "Epoch 72/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9756 - loss: 0.3204 - val_accuracy: 0.9438 - val_loss: 0.4104\n",
            "Epoch 73/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9768 - loss: 0.3174 - val_accuracy: 0.9438 - val_loss: 0.4150\n",
            "Epoch 74/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9828 - loss: 0.2916 - val_accuracy: 0.9438 - val_loss: 0.4109\n",
            "Epoch 75/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9684 - loss: 0.3515 - val_accuracy: 0.9438 - val_loss: 0.4218\n",
            "Epoch 76/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9889 - loss: 0.2874 - val_accuracy: 0.9500 - val_loss: 0.4098\n",
            "Epoch 77/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9810 - loss: 0.3035 - val_accuracy: 0.9500 - val_loss: 0.3930\n",
            "Epoch 78/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9819 - loss: 0.2809 - val_accuracy: 0.9438 - val_loss: 0.3804\n",
            "Epoch 79/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9650 - loss: 0.3194 - val_accuracy: 0.9625 - val_loss: 0.3731\n",
            "Epoch 80/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9729 - loss: 0.2999 - val_accuracy: 0.9563 - val_loss: 0.3833\n",
            "Epoch 81/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9794 - loss: 0.2800 - val_accuracy: 0.9500 - val_loss: 0.3897\n",
            "Epoch 82/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9681 - loss: 0.3072 - val_accuracy: 0.9563 - val_loss: 0.3913\n",
            "Epoch 83/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9779 - loss: 0.2782 - val_accuracy: 0.9625 - val_loss: 0.3950\n",
            "Epoch 84/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9779 - loss: 0.2780 - val_accuracy: 0.9563 - val_loss: 0.3866\n",
            "Epoch 85/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9842 - loss: 0.2626 - val_accuracy: 0.9438 - val_loss: 0.3886\n",
            "Epoch 86/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9591 - loss: 0.3173 - val_accuracy: 0.9500 - val_loss: 0.3851\n",
            "Epoch 87/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9845 - loss: 0.2766 - val_accuracy: 0.9500 - val_loss: 0.3680\n",
            "Epoch 88/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9834 - loss: 0.2594 - val_accuracy: 0.9500 - val_loss: 0.3757\n",
            "Epoch 89/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9849 - loss: 0.2686 - val_accuracy: 0.9500 - val_loss: 0.3808\n",
            "Epoch 90/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9886 - loss: 0.2404 - val_accuracy: 0.9438 - val_loss: 0.3842\n",
            "Epoch 91/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9769 - loss: 0.2708 - val_accuracy: 0.9438 - val_loss: 0.3981\n",
            "Epoch 92/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9755 - loss: 0.2784 - val_accuracy: 0.9563 - val_loss: 0.3802\n",
            "Epoch 93/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9842 - loss: 0.2602 - val_accuracy: 0.9500 - val_loss: 0.3811\n",
            "Epoch 94/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9862 - loss: 0.2490 - val_accuracy: 0.9438 - val_loss: 0.3620\n",
            "Epoch 95/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9801 - loss: 0.2516 - val_accuracy: 0.9563 - val_loss: 0.3604\n",
            "Epoch 96/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9717 - loss: 0.3093 - val_accuracy: 0.9438 - val_loss: 0.3805\n",
            "Epoch 97/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9796 - loss: 0.2419 - val_accuracy: 0.9438 - val_loss: 0.3626\n",
            "Epoch 98/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9830 - loss: 0.2461 - val_accuracy: 0.9500 - val_loss: 0.3589\n",
            "Epoch 99/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9777 - loss: 0.2574 - val_accuracy: 0.9500 - val_loss: 0.3466\n",
            "Epoch 100/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9766 - loss: 0.2407 - val_accuracy: 0.9438 - val_loss: 0.3555\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9637 - loss: 0.2920  \n",
            "\n",
            "✅ Test Accuracy: 0.9600\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.losses import BinaryCrossentropy\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Generate synthetic binary classification data\n",
        "X, y = make_classification(n_samples=1000, n_features=20,\n",
        "                           n_informative=15, n_redundant=5,\n",
        "                           random_state=42)\n",
        "\n",
        "# Normalize features\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the model\n",
        "model = Sequential([\n",
        "    Dense(128, activation='relu', kernel_initializer='he_normal', kernel_regularizer=tf.keras.regularizers.l2(0.001), input_shape=(X.shape[1],)),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.3),\n",
        "\n",
        "    Dense(64, activation='relu', kernel_initializer='he_normal', kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.3),\n",
        "\n",
        "    Dense(32, activation='relu', kernel_initializer='he_normal', kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.2),\n",
        "\n",
        "    Dense(1, activation='sigmoid')  # Output layer for binary classification\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=Adam(learning_rate=0.001),\n",
        "              loss=BinaryCrossentropy(),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Early stopping callback\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train, y_train,\n",
        "                    epochs=100,\n",
        "                    batch_size=32,\n",
        "                    validation_split=0.2,\n",
        "                    callbacks=[early_stop],\n",
        "                    verbose=1)\n",
        "\n",
        "# Evaluate the model\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(f\"\\n✅ Test Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Plot training & validation loss\n",
        "plt.plot(history.history['loss'], label='Train Loss')\n",
        "plt.plot(history.history['val_loss'], label='Val Loss')\n",
        "plt.title(\"Loss Curve with Regularization and Dropout\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 📘 1. Basics of Neural Networks\n",
        "# What is a Neural Network? – No code required\n",
        "\n",
        "# Perceptron Model\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "import numpy as np\n",
        "\n",
        "# Dummy data for Perceptron (AND logic)\n",
        "X = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
        "y = np.array([0, 0, 0, 1])\n",
        "\n",
        "model = Sequential([Dense(1, input_shape=(2,), activation='sigmoid')])\n",
        "model.compile(optimizer='sgd', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "model.fit(X, y, epochs=100)\n",
        "\n",
        "# Multi-layer Perceptron (MLP)\n",
        "model_mlp = Sequential([\n",
        "    Dense(4, input_shape=(2,), activation='relu'),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "model_mlp.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "model_mlp.fit(X, y, epochs=100)\n",
        "\n",
        "# 🧮 3. Forward Propagation – Shown above through model execution\n",
        "\n",
        "# 🔁 4. Activation Functions\n",
        "from tensorflow.keras.layers import Activation, LeakyReLU\n",
        "\n",
        "# Different activation usage in Keras\n",
        "activation_model = Sequential([\n",
        "    Dense(4, input_shape=(2,)),\n",
        "    Activation('tanh'),\n",
        "    Dense(1),\n",
        "    Activation('sigmoid')\n",
        "])\n",
        "\n",
        "# Leaky ReLU / PReLU\n",
        "from tensorflow.keras.layers import PReLU\n",
        "leaky_relu_model = Sequential([\n",
        "    Dense(4, input_shape=(2,)),\n",
        "    LeakyReLU(alpha=0.1),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# ELU, Swish (advanced)\n",
        "from tensorflow.keras.activations import elu, swish\n",
        "\n",
        "# Use custom activation\n",
        "custom_model = Sequential([\n",
        "    Dense(4, input_shape=(2,), activation=swish),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# 📉 5. Loss Functions\n",
        "# Already shown with binary_crossentropy, use categorical_crossentropy or mse similarly\n",
        "\n",
        "# 🧠 6. Backpropagation – Handled internally by Keras\n",
        "\n",
        "# 🚀 7. Optimization Algorithms\n",
        "from tensorflow.keras.optimizers import SGD, Adam, RMSprop\n",
        "\n",
        "opt_model = Sequential([\n",
        "    Dense(4, input_shape=(2,), activation='relu'),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "opt_model.compile(optimizer=RMSprop(), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# 🧩 8. Weight Initialization\n",
        "from tensorflow.keras.initializers import RandomNormal, HeNormal, GlorotUniform\n",
        "\n",
        "init_model = Sequential([\n",
        "    Dense(4, input_shape=(2,), activation='relu', kernel_initializer=HeNormal()),\n",
        "    Dense(1, activation='sigmoid', kernel_initializer=GlorotUniform())\n",
        "])\n",
        "\n",
        "# 🎯 9. Regularization Techniques\n",
        "from tensorflow.keras.regularizers import l1, l2\n",
        "from tensorflow.keras.layers import Dropout\n",
        "\n",
        "reg_model = Sequential([\n",
        "    Dense(4, input_shape=(2,), activation='relu', kernel_regularizer=l2(0.01)),\n",
        "    Dropout(0.5),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# 📊 10. Normalization\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "\n",
        "norm_model = Sequential([\n",
        "    Dense(4, input_shape=(2,)),\n",
        "    BatchNormalization(),\n",
        "    Activation('relu'),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# 🧪 11. Training Techniques\n",
        "# Already using Train/Test in fit().\n",
        "\n",
        "# 🔍 12. Evaluation Metrics\n",
        "# accuracy, precision, recall, f1 – use sklearn\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "# After prediction\n",
        "# preds = model.predict(X)\n",
        "# print(confusion_matrix(y, preds.round()))\n",
        "\n",
        "# 🛠️ 13. Practical Techniques\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "callbacks = [\n",
        "    EarlyStopping(patience=5, monitor='val_loss'),\n",
        "    ModelCheckpoint('best_model.h5', save_best_only=True)\n",
        "]\n",
        "model.fit(X, y, epochs=100, validation_split=0.2, callbacks=callbacks)\n",
        "\n",
        "# 🧰 14. Frameworks & Implementation\n",
        "# NumPy implementation would be manual – not shown here\n",
        "# TensorFlow and Keras used above\n",
        "\n",
        "# 🧠 15. Advanced Concepts\n",
        "# Autoencoders\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input\n",
        "\n",
        "input_layer = Input(shape=(32,))\n",
        "encoded = Dense(16, activation='relu')(input_layer)\n",
        "decoded = Dense(32, activation='sigmoid')(encoded)\n",
        "autoencoder = Model(input_layer, decoded)\n",
        "autoencoder.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "# Transfer Learning using pretrained dense layers – typically using larger models, not shown here\n",
        "\n",
        "# 📦 Bonus: Real Estate Price Prediction (Regression)\n",
        "from sklearn.datasets import make_regression\n",
        "X, y = make_regression(n_samples=1000, n_features=10, noise=0.1)\n",
        "\n",
        "model_reg = Sequential([\n",
        "    Dense(64, activation='relu', input_shape=(10,)),\n",
        "    Dense(1)\n",
        "])\n",
        "model_reg.compile(optimizer='adam', loss='mse')\n",
        "model_reg.fit(X, y, epochs=50, batch_size=32)\n",
        "\n",
        "# Similarly for binary classification (spam), sentiment analysis, etc.\n",
        "# You can extend this template for MNIST and other datasets using keras.datasets\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CRLL2Vzd4ziF",
        "outputId": "e6be323d-aee3-45d0-ce8d-25763a269d5b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 541ms/step - accuracy: 0.5000 - loss: 0.7546\n",
            "Epoch 2/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.5000 - loss: 0.7529\n",
            "Epoch 3/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.5000 - loss: 0.7513\n",
            "Epoch 4/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.5000 - loss: 0.7497\n",
            "Epoch 5/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.5000 - loss: 0.7482\n",
            "Epoch 6/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step - accuracy: 0.5000 - loss: 0.7466\n",
            "Epoch 7/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.5000 - loss: 0.7450\n",
            "Epoch 8/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.5000 - loss: 0.7435\n",
            "Epoch 9/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.5000 - loss: 0.7419\n",
            "Epoch 10/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.5000 - loss: 0.7404\n",
            "Epoch 11/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.5000 - loss: 0.7388\n",
            "Epoch 12/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.5000 - loss: 0.7373\n",
            "Epoch 13/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.5000 - loss: 0.7358\n",
            "Epoch 14/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.5000 - loss: 0.7343\n",
            "Epoch 15/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.5000 - loss: 0.7328\n",
            "Epoch 16/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - accuracy: 0.5000 - loss: 0.7313\n",
            "Epoch 17/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.5000 - loss: 0.7299\n",
            "Epoch 18/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.5000 - loss: 0.7284\n",
            "Epoch 19/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.5000 - loss: 0.7270\n",
            "Epoch 20/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.5000 - loss: 0.7255\n",
            "Epoch 21/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.5000 - loss: 0.7241\n",
            "Epoch 22/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.5000 - loss: 0.7226\n",
            "Epoch 23/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.5000 - loss: 0.7212\n",
            "Epoch 24/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.5000 - loss: 0.7198\n",
            "Epoch 25/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.5000 - loss: 0.7184\n",
            "Epoch 26/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.5000 - loss: 0.7170\n",
            "Epoch 27/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.5000 - loss: 0.7156\n",
            "Epoch 28/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.5000 - loss: 0.7143\n",
            "Epoch 29/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.5000 - loss: 0.7129\n",
            "Epoch 30/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.5000 - loss: 0.7115\n",
            "Epoch 31/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.5000 - loss: 0.7102\n",
            "Epoch 32/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.5000 - loss: 0.7088\n",
            "Epoch 33/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.5000 - loss: 0.7075\n",
            "Epoch 34/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.5000 - loss: 0.7062\n",
            "Epoch 35/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.5000 - loss: 0.7049\n",
            "Epoch 36/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.5000 - loss: 0.7035\n",
            "Epoch 37/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.5000 - loss: 0.7022\n",
            "Epoch 38/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.5000 - loss: 0.7010\n",
            "Epoch 39/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.5000 - loss: 0.6997\n",
            "Epoch 40/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.5000 - loss: 0.6984\n",
            "Epoch 41/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.5000 - loss: 0.6971\n",
            "Epoch 42/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.5000 - loss: 0.6959\n",
            "Epoch 43/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.5000 - loss: 0.6946\n",
            "Epoch 44/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.5000 - loss: 0.6934\n",
            "Epoch 45/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.5000 - loss: 0.6921\n",
            "Epoch 46/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.5000 - loss: 0.6909\n",
            "Epoch 47/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.5000 - loss: 0.6897\n",
            "Epoch 48/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.5000 - loss: 0.6885\n",
            "Epoch 49/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.5000 - loss: 0.6873\n",
            "Epoch 50/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.5000 - loss: 0.6861\n",
            "Epoch 51/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.5000 - loss: 0.6849\n",
            "Epoch 52/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.5000 - loss: 0.6837\n",
            "Epoch 53/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.7500 - loss: 0.6825\n",
            "Epoch 54/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.7500 - loss: 0.6813\n",
            "Epoch 55/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - accuracy: 0.7500 - loss: 0.6802\n",
            "Epoch 56/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.7500 - loss: 0.6790\n",
            "Epoch 57/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.7500 - loss: 0.6779\n",
            "Epoch 58/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.7500 - loss: 0.6767\n",
            "Epoch 59/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.7500 - loss: 0.6756\n",
            "Epoch 60/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.7500 - loss: 0.6745\n",
            "Epoch 61/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.7500 - loss: 0.6733\n",
            "Epoch 62/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.7500 - loss: 0.6722\n",
            "Epoch 63/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.7500 - loss: 0.6711\n",
            "Epoch 64/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.7500 - loss: 0.6700\n",
            "Epoch 65/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.7500 - loss: 0.6689\n",
            "Epoch 66/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.7500 - loss: 0.6679\n",
            "Epoch 67/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.7500 - loss: 0.6668\n",
            "Epoch 68/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.7500 - loss: 0.6657\n",
            "Epoch 69/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.7500 - loss: 0.6646\n",
            "Epoch 70/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.7500 - loss: 0.6636\n",
            "Epoch 71/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 206ms/step - accuracy: 0.7500 - loss: 0.6625\n",
            "Epoch 72/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step - accuracy: 0.7500 - loss: 0.6615\n",
            "Epoch 73/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - accuracy: 0.7500 - loss: 0.6604\n",
            "Epoch 74/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.7500 - loss: 0.6594\n",
            "Epoch 75/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.7500 - loss: 0.6584\n",
            "Epoch 76/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.7500 - loss: 0.6574\n",
            "Epoch 77/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.7500 - loss: 0.6564\n",
            "Epoch 78/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.7500 - loss: 0.6553\n",
            "Epoch 79/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.7500 - loss: 0.6543\n",
            "Epoch 80/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.7500 - loss: 0.6533\n",
            "Epoch 81/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - accuracy: 0.7500 - loss: 0.6524\n",
            "Epoch 82/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 226ms/step - accuracy: 0.7500 - loss: 0.6514\n",
            "Epoch 83/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 224ms/step - accuracy: 0.7500 - loss: 0.6504\n",
            "Epoch 84/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 0.7500 - loss: 0.6494\n",
            "Epoch 85/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - accuracy: 0.7500 - loss: 0.6485\n",
            "Epoch 86/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.7500 - loss: 0.6475\n",
            "Epoch 87/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 267ms/step - accuracy: 0.7500 - loss: 0.6466\n",
            "Epoch 88/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 318ms/step - accuracy: 0.7500 - loss: 0.6456\n",
            "Epoch 89/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 172ms/step - accuracy: 0.7500 - loss: 0.6447\n",
            "Epoch 90/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.7500 - loss: 0.6437\n",
            "Epoch 91/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - accuracy: 0.7500 - loss: 0.6428\n",
            "Epoch 92/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step - accuracy: 0.7500 - loss: 0.6419\n",
            "Epoch 93/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - accuracy: 0.7500 - loss: 0.6410\n",
            "Epoch 94/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 146ms/step - accuracy: 0.7500 - loss: 0.6401\n",
            "Epoch 95/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step - accuracy: 0.7500 - loss: 0.6391\n",
            "Epoch 96/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 148ms/step - accuracy: 0.7500 - loss: 0.6382\n",
            "Epoch 97/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step - accuracy: 0.7500 - loss: 0.6374\n",
            "Epoch 98/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.7500 - loss: 0.6365\n",
            "Epoch 99/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.7500 - loss: 0.6356\n",
            "Epoch 100/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - accuracy: 0.7500 - loss: 0.6347\n",
            "Epoch 1/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - accuracy: 0.5000 - loss: 0.7392\n",
            "Epoch 2/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 0.5000 - loss: 0.7382\n",
            "Epoch 3/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.5000 - loss: 0.7373\n",
            "Epoch 4/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.5000 - loss: 0.7364\n",
            "Epoch 5/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.5000 - loss: 0.7354\n",
            "Epoch 6/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.5000 - loss: 0.7345\n",
            "Epoch 7/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.5000 - loss: 0.7336\n",
            "Epoch 8/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.5000 - loss: 0.7326\n",
            "Epoch 9/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.5000 - loss: 0.7317\n",
            "Epoch 10/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.5000 - loss: 0.7308\n",
            "Epoch 11/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.5000 - loss: 0.7299\n",
            "Epoch 12/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.5000 - loss: 0.7290\n",
            "Epoch 13/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.5000 - loss: 0.7280\n",
            "Epoch 14/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.5000 - loss: 0.7271\n",
            "Epoch 15/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.5000 - loss: 0.7262\n",
            "Epoch 16/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.5000 - loss: 0.7253\n",
            "Epoch 17/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.5000 - loss: 0.7244\n",
            "Epoch 18/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - accuracy: 0.5000 - loss: 0.7235\n",
            "Epoch 19/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.5000 - loss: 0.7226\n",
            "Epoch 20/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.5000 - loss: 0.7217\n",
            "Epoch 21/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.5000 - loss: 0.7208\n",
            "Epoch 22/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.5000 - loss: 0.7199\n",
            "Epoch 23/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.5000 - loss: 0.7191\n",
            "Epoch 24/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.5000 - loss: 0.7182\n",
            "Epoch 25/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - accuracy: 0.5000 - loss: 0.7173\n",
            "Epoch 26/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.5000 - loss: 0.7164\n",
            "Epoch 27/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.5000 - loss: 0.7155\n",
            "Epoch 28/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.7500 - loss: 0.7147\n",
            "Epoch 29/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.7500 - loss: 0.7138\n",
            "Epoch 30/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.7500 - loss: 0.7129\n",
            "Epoch 31/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.7500 - loss: 0.7120\n",
            "Epoch 32/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.7500 - loss: 0.7112\n",
            "Epoch 33/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.7500 - loss: 0.7103\n",
            "Epoch 34/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step - accuracy: 0.7500 - loss: 0.7095\n",
            "Epoch 35/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.7500 - loss: 0.7086\n",
            "Epoch 36/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.7500 - loss: 0.7077\n",
            "Epoch 37/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.7500 - loss: 0.7069\n",
            "Epoch 38/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.7500 - loss: 0.7060\n",
            "Epoch 39/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.7500 - loss: 0.7052\n",
            "Epoch 40/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.7500 - loss: 0.7043\n",
            "Epoch 41/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.7500 - loss: 0.7035\n",
            "Epoch 42/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.7500 - loss: 0.7026\n",
            "Epoch 43/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.7500 - loss: 0.7018\n",
            "Epoch 44/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.7500 - loss: 0.7010\n",
            "Epoch 45/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.7500 - loss: 0.7001\n",
            "Epoch 46/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step - accuracy: 0.7500 - loss: 0.6993\n",
            "Epoch 47/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.7500 - loss: 0.6984\n",
            "Epoch 48/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.7500 - loss: 0.6976\n",
            "Epoch 49/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - accuracy: 0.7500 - loss: 0.6968\n",
            "Epoch 50/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.7500 - loss: 0.6959\n",
            "Epoch 51/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.7500 - loss: 0.6951\n",
            "Epoch 52/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.7500 - loss: 0.6943\n",
            "Epoch 53/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.7500 - loss: 0.6934\n",
            "Epoch 54/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.7500 - loss: 0.6926\n",
            "Epoch 55/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.7500 - loss: 0.6918\n",
            "Epoch 56/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.7500 - loss: 0.6910\n",
            "Epoch 57/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.7500 - loss: 0.6901\n",
            "Epoch 58/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.7500 - loss: 0.6893\n",
            "Epoch 59/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.7500 - loss: 0.6885\n",
            "Epoch 60/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.7500 - loss: 0.6877\n",
            "Epoch 61/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.7500 - loss: 0.6869\n",
            "Epoch 62/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.7500 - loss: 0.6860\n",
            "Epoch 63/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.7500 - loss: 0.6852\n",
            "Epoch 64/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step - accuracy: 0.7500 - loss: 0.6844\n",
            "Epoch 65/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.7500 - loss: 0.6836\n",
            "Epoch 66/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.7500 - loss: 0.6828\n",
            "Epoch 67/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.7500 - loss: 0.6819\n",
            "Epoch 68/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.7500 - loss: 0.6811\n",
            "Epoch 69/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.7500 - loss: 0.6803\n",
            "Epoch 70/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.7500 - loss: 0.6795\n",
            "Epoch 71/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.7500 - loss: 0.6787\n",
            "Epoch 72/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.7500 - loss: 0.6779\n",
            "Epoch 73/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.7500 - loss: 0.6771\n",
            "Epoch 74/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step - accuracy: 0.7500 - loss: 0.6762\n",
            "Epoch 75/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.7500 - loss: 0.6754\n",
            "Epoch 76/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.7500 - loss: 0.6746\n",
            "Epoch 77/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.7500 - loss: 0.6738\n",
            "Epoch 78/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - accuracy: 0.7500 - loss: 0.6730\n",
            "Epoch 79/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.7500 - loss: 0.6722\n",
            "Epoch 80/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.7500 - loss: 0.6714\n",
            "Epoch 81/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.7500 - loss: 0.6706\n",
            "Epoch 82/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.7500 - loss: 0.6697\n",
            "Epoch 83/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.7500 - loss: 0.6689\n",
            "Epoch 84/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.7500 - loss: 0.6681\n",
            "Epoch 85/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.7500 - loss: 0.6673\n",
            "Epoch 86/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.7500 - loss: 0.6665\n",
            "Epoch 87/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.7500 - loss: 0.6657\n",
            "Epoch 88/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - accuracy: 0.7500 - loss: 0.6649\n",
            "Epoch 89/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - accuracy: 0.7500 - loss: 0.6641\n",
            "Epoch 90/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - accuracy: 0.7500 - loss: 0.6633\n",
            "Epoch 91/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - accuracy: 0.7500 - loss: 0.6624\n",
            "Epoch 92/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.7500 - loss: 0.6616\n",
            "Epoch 93/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.7500 - loss: 0.6608\n",
            "Epoch 94/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.7500 - loss: 0.6600\n",
            "Epoch 95/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.7500 - loss: 0.6592\n",
            "Epoch 96/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.7500 - loss: 0.6584\n",
            "Epoch 97/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.7500 - loss: 0.6576\n",
            "Epoch 98/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.7500 - loss: 0.6568\n",
            "Epoch 99/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.7500 - loss: 0.6559\n",
            "Epoch 100/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.7500 - loss: 0.6551\n",
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/activations/leaky_relu.py:41: UserWarning: Argument `alpha` is deprecated. Use `negative_slope` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.6667 - loss: 0.7027"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 308ms/step - accuracy: 0.6667 - loss: 0.7027 - val_accuracy: 1.0000 - val_loss: 0.4301\n",
            "Epoch 2/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 407ms/step - accuracy: 0.6667 - loss: 0.6996 - val_accuracy: 1.0000 - val_loss: 0.4331\n",
            "Epoch 3/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step - accuracy: 0.6667 - loss: 0.6965 - val_accuracy: 1.0000 - val_loss: 0.4361\n",
            "Epoch 4/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step - accuracy: 0.6667 - loss: 0.6934 - val_accuracy: 1.0000 - val_loss: 0.4391\n",
            "Epoch 5/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step - accuracy: 0.6667 - loss: 0.6904 - val_accuracy: 1.0000 - val_loss: 0.4421\n",
            "Epoch 6/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step - accuracy: 0.6667 - loss: 0.6873 - val_accuracy: 1.0000 - val_loss: 0.4451\n",
            "Epoch 1/50\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 19341.3691\n",
            "Epoch 2/50\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 19717.4727\n",
            "Epoch 3/50\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 19414.1973\n",
            "Epoch 4/50\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 20174.9902\n",
            "Epoch 5/50\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 19530.2246\n",
            "Epoch 6/50\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 18326.9082\n",
            "Epoch 7/50\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 15980.7432\n",
            "Epoch 8/50\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 17542.8848\n",
            "Epoch 9/50\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 16654.1875\n",
            "Epoch 10/50\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 15628.4590\n",
            "Epoch 11/50\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 14712.2529\n",
            "Epoch 12/50\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 13617.5166\n",
            "Epoch 13/50\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 12842.9316\n",
            "Epoch 14/50\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 10768.3779\n",
            "Epoch 15/50\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 10208.2910\n",
            "Epoch 16/50\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 9205.2061\n",
            "Epoch 17/50\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7597.6538\n",
            "Epoch 18/50\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7216.5757\n",
            "Epoch 19/50\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7132.6162\n",
            "Epoch 20/50\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5450.2446\n",
            "Epoch 21/50\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4946.2476\n",
            "Epoch 22/50\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4004.3796\n",
            "Epoch 23/50\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3298.5271\n",
            "Epoch 24/50\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2565.4341\n",
            "Epoch 25/50\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2379.6907\n",
            "Epoch 26/50\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1866.1217\n",
            "Epoch 27/50\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1743.0448\n",
            "Epoch 28/50\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1478.4712\n",
            "Epoch 29/50\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1128.1100\n",
            "Epoch 30/50\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 952.0912\n",
            "Epoch 31/50\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 768.4068\n",
            "Epoch 32/50\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 740.2294\n",
            "Epoch 33/50\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 647.1876\n",
            "Epoch 34/50\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 538.8199\n",
            "Epoch 35/50\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 523.5353\n",
            "Epoch 36/50\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 446.6437\n",
            "Epoch 37/50\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 415.5120\n",
            "Epoch 38/50\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 415.4804\n",
            "Epoch 39/50\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 391.4026\n",
            "Epoch 40/50\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 347.5096\n",
            "Epoch 41/50\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 370.9184\n",
            "Epoch 42/50\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 358.9525\n",
            "Epoch 43/50\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 327.1871\n",
            "Epoch 44/50\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 325.2583\n",
            "Epoch 45/50\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 321.3672\n",
            "Epoch 46/50\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 304.4719\n",
            "Epoch 47/50\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 295.3073\n",
            "Epoch 48/50\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 291.1603\n",
            "Epoch 49/50\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 282.7401\n",
            "Epoch 50/50\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 292.5829\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7a61eaafd6d0>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### perceptorn"
      ],
      "metadata": {
        "id": "ArTO2w5e59nv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class Perceptron:\n",
        "    def __init__(self, learning_rate=0.01, n_iters=1000):\n",
        "        self.lr = learning_rate\n",
        "        self.n_iters = n_iters\n",
        "        self.activation_func = self._step_function\n",
        "        self.weights = None\n",
        "        self.bias = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        # Initialize weights and bias\n",
        "        n_samples, n_features = X.shape\n",
        "        self.weights = np.zeros(n_features)\n",
        "        self.bias = 0\n",
        "\n",
        "        # Training loop\n",
        "        for _ in range(self.n_iters):\n",
        "            for idx, x_i in enumerate(X):\n",
        "                linear_output = np.dot(x_i, self.weights) + self.bias\n",
        "                y_pred = self.activation_func(linear_output)\n",
        "\n",
        "                # Perceptron weight update rule\n",
        "                update = self.lr * (y[idx] - y_pred)\n",
        "                self.weights += update * x_i\n",
        "                self.bias += update\n",
        "\n",
        "    def predict(self, X):\n",
        "        linear_output = np.dot(X, self.weights) + self.bias\n",
        "        return self.activation_func(linear_output)\n",
        "\n",
        "    def _step_function(self, x):\n",
        "        return np.where(x >= 0, 1, 0)\n",
        "\n",
        "# Example: AND gate\n",
        "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
        "y = np.array([0, 0, 0, 1])  # Output of AND gate\n",
        "\n",
        "p = Perceptron(learning_rate=0.1, n_iters=10)\n",
        "p.fit(X, y)\n",
        "\n",
        "print(\"Predictions:\", p.predict(X))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9MwDNCNE58Q_",
        "outputId": "290a88af-6cd1-4ecc-b256-67a59a8ded16"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predictions: [0 0 0 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import Perceptron\n",
        "import numpy as np\n",
        "\n",
        "# Define input features (AND gate)\n",
        "X = np.array([\n",
        "    [0, 0],\n",
        "    [0, 1],\n",
        "    [1, 0],\n",
        "    [1, 1]\n",
        "])\n",
        "\n",
        "# Output labels (AND gate output)\n",
        "y = np.array([0, 0, 0, 1])\n",
        "\n",
        "# Initialize and train the Perceptron model\n",
        "model = Perceptron(max_iter=1000, eta0=0.1, random_state=42)\n",
        "\n",
        "# Fit model to the data\n",
        "model.fit(X, y)\n",
        "\n",
        "# Predict on the same inputs\n",
        "predictions = model.predict(X)\n",
        "\n",
        "# Display predictions\n",
        "print(\"Predictions:\", predictions)\n",
        "print(\"Weights:\", model.coef_)\n",
        "print(\"Bias (intercept):\", model.intercept_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KzeplYYb6G8D",
        "outputId": "4040d584-08a0-4ce5-bdc4-6b78b2c8fe4f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predictions: [0 0 0 1]\n",
            "Weights: [[0.2 0.2]]\n",
            "Bias (intercept): [-0.2]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "# Input data (AND gate)\n",
        "X = np.array([\n",
        "    [0, 0],\n",
        "    [0, 1],\n",
        "    [1, 0],\n",
        "    [1, 1]\n",
        "])\n",
        "\n",
        "# Labels\n",
        "y = np.array([0, 0, 0, 1])\n",
        "\n",
        "# Build the model\n",
        "model = Sequential()\n",
        "\n",
        "# Add a single dense layer with 1 neuron (this acts like a perceptron)\n",
        "# Use 'sigmoid' activation for binary classification\n",
        "model.add(Dense(1, input_dim=2, activation='sigmoid'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='sgd', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(X, y, epochs=100, verbose=0)\n",
        "\n",
        "# Evaluate performance\n",
        "loss, accuracy = model.evaluate(X, y, verbose=0)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n",
        "# Predict\n",
        "predictions = model.predict(X)\n",
        "predicted_labels = (predictions > 0.5).astype(int)\n",
        "\n",
        "print(\"Predictions (probabilities):\", predictions.ravel())\n",
        "print(\"Predicted Classes:\", predicted_labels.ravel())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F3GuQ4yG6J4e",
        "outputId": "7b04733f-b55e-482f-a020-20b34a7dd2d1"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.5\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step\n",
            "Predictions (probabilities): [0.4444597  0.17893639 0.7435657  0.441296  ]\n",
            "Predicted Classes: [0 0 1 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Perceptron Trick | How to train a Perceptron | Perceptron Part 2 |\n"
      ],
      "metadata": {
        "id": "kT1FRBdJ6XHn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class Perceptron:\n",
        "    def __init__(self, learning_rate=0.1, n_iters=100):\n",
        "        self.lr = learning_rate\n",
        "        self.n_iters = n_iters\n",
        "        self.activation = self._step_function  # Binary activation function\n",
        "        self.weights = None\n",
        "        self.bias = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        n_samples, n_features = X.shape\n",
        "\n",
        "        # Initialize weights and bias to zero\n",
        "        self.weights = np.zeros(n_features)\n",
        "        self.bias = 0\n",
        "\n",
        "        # Training loop\n",
        "        for _ in range(self.n_iters):\n",
        "            for idx, x_i in enumerate(X):\n",
        "                linear_output = np.dot(x_i, self.weights) + self.bias\n",
        "                y_pred = self.activation(linear_output)\n",
        "\n",
        "                # Perceptron trick (update rule)\n",
        "                update = self.lr * (y[idx] - y_pred)\n",
        "\n",
        "                # Update weights and bias\n",
        "                self.weights += update * x_i\n",
        "                self.bias += update\n",
        "\n",
        "                # Debugging/understanding step\n",
        "                print(f\"Update: {update}, Weights: {self.weights}, Bias: {self.bias}\")\n",
        "\n",
        "    def predict(self, X):\n",
        "        linear_output = np.dot(X, self.weights) + self.bias\n",
        "        return self.activation(linear_output)\n",
        "\n",
        "    def _step_function(self, x):\n",
        "        return np.where(x >= 0, 1, 0)\n",
        "\n",
        "# AND gate example\n",
        "X = np.array([\n",
        "    [0, 0],\n",
        "    [0, 1],\n",
        "    [1, 0],\n",
        "    [1, 1]\n",
        "])\n",
        "y = np.array([0, 0, 0, 1])  # Output of AND\n",
        "\n",
        "# Create and train model\n",
        "p = Perceptron(learning_rate=0.1, n_iters=10)\n",
        "p.fit(X, y)\n",
        "\n",
        "# Predict on training data\n",
        "print(\"Final Predictions:\", p.predict(X))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d45fbNM56YRs",
        "outputId": "7f5d12b2-cfd4-4141-9f12-aee7ce7f36d5"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Update: -0.1, Weights: [0. 0.], Bias: -0.1\n",
            "Update: 0.0, Weights: [0. 0.], Bias: -0.1\n",
            "Update: 0.0, Weights: [0. 0.], Bias: -0.1\n",
            "Update: 0.1, Weights: [0.1 0.1], Bias: 0.0\n",
            "Update: -0.1, Weights: [0.1 0.1], Bias: -0.1\n",
            "Update: -0.1, Weights: [0.1 0. ], Bias: -0.2\n",
            "Update: 0.0, Weights: [0.1 0. ], Bias: -0.2\n",
            "Update: 0.1, Weights: [0.2 0.1], Bias: -0.1\n",
            "Update: 0.0, Weights: [0.2 0.1], Bias: -0.1\n",
            "Update: -0.1, Weights: [0.2 0. ], Bias: -0.2\n",
            "Update: -0.1, Weights: [0.1 0. ], Bias: -0.30000000000000004\n",
            "Update: 0.1, Weights: [0.2 0.1], Bias: -0.20000000000000004\n",
            "Update: 0.0, Weights: [0.2 0.1], Bias: -0.20000000000000004\n",
            "Update: 0.0, Weights: [0.2 0.1], Bias: -0.20000000000000004\n",
            "Update: 0.0, Weights: [0.2 0.1], Bias: -0.20000000000000004\n",
            "Update: 0.0, Weights: [0.2 0.1], Bias: -0.20000000000000004\n",
            "Update: 0.0, Weights: [0.2 0.1], Bias: -0.20000000000000004\n",
            "Update: 0.0, Weights: [0.2 0.1], Bias: -0.20000000000000004\n",
            "Update: 0.0, Weights: [0.2 0.1], Bias: -0.20000000000000004\n",
            "Update: 0.0, Weights: [0.2 0.1], Bias: -0.20000000000000004\n",
            "Update: 0.0, Weights: [0.2 0.1], Bias: -0.20000000000000004\n",
            "Update: 0.0, Weights: [0.2 0.1], Bias: -0.20000000000000004\n",
            "Update: 0.0, Weights: [0.2 0.1], Bias: -0.20000000000000004\n",
            "Update: 0.0, Weights: [0.2 0.1], Bias: -0.20000000000000004\n",
            "Update: 0.0, Weights: [0.2 0.1], Bias: -0.20000000000000004\n",
            "Update: 0.0, Weights: [0.2 0.1], Bias: -0.20000000000000004\n",
            "Update: 0.0, Weights: [0.2 0.1], Bias: -0.20000000000000004\n",
            "Update: 0.0, Weights: [0.2 0.1], Bias: -0.20000000000000004\n",
            "Update: 0.0, Weights: [0.2 0.1], Bias: -0.20000000000000004\n",
            "Update: 0.0, Weights: [0.2 0.1], Bias: -0.20000000000000004\n",
            "Update: 0.0, Weights: [0.2 0.1], Bias: -0.20000000000000004\n",
            "Update: 0.0, Weights: [0.2 0.1], Bias: -0.20000000000000004\n",
            "Update: 0.0, Weights: [0.2 0.1], Bias: -0.20000000000000004\n",
            "Update: 0.0, Weights: [0.2 0.1], Bias: -0.20000000000000004\n",
            "Update: 0.0, Weights: [0.2 0.1], Bias: -0.20000000000000004\n",
            "Update: 0.0, Weights: [0.2 0.1], Bias: -0.20000000000000004\n",
            "Update: 0.0, Weights: [0.2 0.1], Bias: -0.20000000000000004\n",
            "Update: 0.0, Weights: [0.2 0.1], Bias: -0.20000000000000004\n",
            "Update: 0.0, Weights: [0.2 0.1], Bias: -0.20000000000000004\n",
            "Update: 0.0, Weights: [0.2 0.1], Bias: -0.20000000000000004\n",
            "Final Predictions: [0 0 0 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import Perceptron\n",
        "import numpy as np\n",
        "\n",
        "# Sample data - AND gate\n",
        "X = np.array([\n",
        "    [0, 0],\n",
        "    [0, 1],\n",
        "    [1, 0],\n",
        "    [1, 1]\n",
        "])\n",
        "y = np.array([0, 0, 0, 1])\n",
        "\n",
        "# Create a perceptron model\n",
        "model = Perceptron(max_iter=10, eta0=0.1, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X, y)\n",
        "\n",
        "# Predict\n",
        "predictions = model.predict(X)\n",
        "\n",
        "print(\"Predictions:\", predictions)\n",
        "print(\"Weights:\", model.coef_)\n",
        "print(\"Bias (intercept):\", model.intercept_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eWrQcFBk6hVg",
        "outputId": "be87796a-8753-420d-9e58-a45b0e0f25ab"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predictions: [0 0 0 1]\n",
            "Weights: [[0.2 0.2]]\n",
            "Bias (intercept): [-0.2]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "# AND gate input\n",
        "X = np.array([\n",
        "    [0, 0],\n",
        "    [0, 1],\n",
        "    [1, 0],\n",
        "    [1, 1]\n",
        "])\n",
        "\n",
        "# AND gate output\n",
        "y = np.array([0, 0, 0, 1])\n",
        "\n",
        "# Create a model: 1 dense layer, 1 neuron, sigmoid activation\n",
        "model = Sequential()\n",
        "model.add(Dense(1, input_dim=2, activation='sigmoid'))\n",
        "\n",
        "# Compile model with binary cross-entropy\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.SGD(learning_rate=0.1),\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Train model\n",
        "model.fit(X, y, epochs=100, verbose=0)\n",
        "\n",
        "# Evaluate\n",
        "loss, acc = model.evaluate(X, y, verbose=0)\n",
        "print(f\"Accuracy: {acc:.2f}\")\n",
        "\n",
        "# Predict\n",
        "predictions = model.predict(X)\n",
        "predicted_classes = (predictions > 0.5).astype(int)\n",
        "\n",
        "print(\"Predictions (Probabilities):\", predictions.ravel())\n",
        "print(\"Predicted Classes:\", predicted_classes.ravel())\n",
        "print(\"Weights:\", model.layers[0].get_weights()[0])\n",
        "print(\"Bias:\", model.layers[0].get_weights()[1])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vVcFlo_G6ojc",
        "outputId": "525875e8-eeb0-4b6c-a690-1139fac3b028"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.75\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step\n",
            "Predictions (Probabilities): [0.21218036 0.45307463 0.20346911 0.43999687]\n",
            "Predicted Classes: [0 0 0 0]\n",
            "Weights: [[-0.05291929]\n",
            " [ 1.123577  ]]\n",
            "Bias: [-1.3118325]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Perceptron Loss Function | Hinge Loss | Binary Cross Entropy | Sigmoid Function\n",
        "\n",
        "🔹 Sigmoid + Binary Crossentropy Loss\n",
        "\n",
        "🔹 Hinge Loss with appropriate activation"
      ],
      "metadata": {
        "id": "_lujDyCq62Hr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "✅ 1. Binary Crossentropy Loss with Sigmoid Activation\n"
      ],
      "metadata": {
        "id": "AdCrKwPH7DLz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "# Input (AND gate)\n",
        "X = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
        "y = np.array([0, 0, 0, 1])  # Binary labels\n",
        "\n",
        "# Build model with sigmoid activation\n",
        "model = Sequential()\n",
        "model.add(Dense(1, input_dim=2, activation='sigmoid'))\n",
        "\n",
        "# Compile with binary crossentropy loss\n",
        "model.compile(optimizer='sgd', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train model\n",
        "model.fit(X, y, epochs=100, verbose=0)\n",
        "\n",
        "# Predict\n",
        "preds = model.predict(X)\n",
        "print(\"Binary Crossentropy Predictions:\", (preds > 0.5).astype(int).ravel())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2WAtOocy627T",
        "outputId": "4d671a47-472c-4766-f1a1-d74aa64d9e43"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step\n",
            "Binary Crossentropy Predictions: [0 1 0 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "✅ 2. Hinge Loss with Tanh Activation\n",
        "\n",
        "Hinge loss requires labels as -1 or +1, and a linear or tanh activation."
      ],
      "metadata": {
        "id": "VDtq0tdf7JhC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "# Input\n",
        "X = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
        "y = np.array([-1, -1, -1, 1])  # Labels for hinge loss must be -1 or 1\n",
        "\n",
        "# Build model using tanh activation (outputs between -1 and 1)\n",
        "model = Sequential()\n",
        "model.add(Dense(1, input_dim=2, activation='tanh'))\n",
        "\n",
        "# Compile with hinge loss\n",
        "model.compile(optimizer='sgd', loss='hinge', metrics=['accuracy'])\n",
        "\n",
        "# Train model\n",
        "model.fit(X, y, epochs=100, verbose=0)\n",
        "\n",
        "# Predict\n",
        "preds = model.predict(X)\n",
        "print(\"Hinge Loss Raw Outputs:\", preds.ravel())\n",
        "print(\"Predicted Labels:\", np.where(preds >= 0, 1, -1))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q8-gQait7K_X",
        "outputId": "58dc4750-3f27-43c3-be20-b05cb4b3c38a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step\n",
            "Hinge Loss Raw Outputs: [-0.16781399  0.78890836 -0.8299717   0.04986399]\n",
            "Predicted Labels: [[-1]\n",
            " [ 1]\n",
            " [-1]\n",
            " [ 1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Multi Layer Perceptron"
      ],
      "metadata": {
        "id": "zeFgMGDL7YjU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "# Sample binary classification data (AND gate)\n",
        "X = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
        "y = np.array([0, 0, 0, 1])\n",
        "\n",
        "# Build Multi-Layer Perceptron\n",
        "model = Sequential()\n",
        "model.add(Dense(4, input_dim=2, activation='relu'))  # Hidden layer\n",
        "model.add(Dense(1, activation='sigmoid'))            # Output layer\n",
        "\n",
        "# Compile\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train\n",
        "model.fit(X, y, epochs=200, verbose=0)\n",
        "\n",
        "# Predict\n",
        "predictions = model.predict(X)\n",
        "print(\"Predicted Classes:\", (predictions > 0.5).astype(int).ravel())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jfwDvGN37ZSU",
        "outputId": "be587b85-ae35-4d6c-ce10-fa5d2c1f4b0a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 5 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7a61e368a840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step\n",
            "Predicted Classes: [0 0 0 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Forward Propagation | How a neural network predicts output?"
      ],
      "metadata": {
        "id": "T7zgrYFD7p_5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "# Input data (e.g., XOR)\n",
        "X = np.array([[0,0], [0,1], [1,0], [1,1]])\n",
        "\n",
        "# Build a simple feedforward neural network (forward propagation)\n",
        "model = Sequential()\n",
        "model.add(Dense(4, input_dim=2, activation='relu'))  # Hidden layer\n",
        "model.add(Dense(1, activation='sigmoid'))            # Output layer\n",
        "\n",
        "# Compile model (required before predict)\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy')\n",
        "\n",
        "# Forward pass (no training, just output calculation)\n",
        "outputs = model(X)  # Forward propagate inputs through network\n",
        "print(\"Forward Propagation Outputs:\", outputs.numpy().ravel())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YR3e-chz7rVM",
        "outputId": "9b3acbdb-fe84-48e0-e5b2-67751fe46a79"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Forward Propagation Outputs: [0.5       0.5435624 0.5       0.5017875]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loss Functions in Deep Learning"
      ],
      "metadata": {
        "id": "Zso-PB3M70lG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "X = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
        "y_binary = np.array([0, 1, 1, 0])        # For binary loss\n",
        "y_categorical = tf.keras.utils.to_categorical([0, 1, 1, 0], num_classes=2)  # For categorical loss\n",
        "\n",
        "# Binary Crossentropy\n",
        "model_bce = Sequential()\n",
        "model_bce.add(Dense(4, input_dim=2, activation='relu'))\n",
        "model_bce.add(Dense(1, activation='sigmoid'))\n",
        "model_bce.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "model_bce.fit(X, y_binary, epochs=100, verbose=0)\n",
        "print(\"Binary Crossentropy:\", model_bce.evaluate(X, y_binary, verbose=0))\n",
        "\n",
        "# Categorical Crossentropy\n",
        "model_cce = Sequential()\n",
        "model_cce.add(Dense(4, input_dim=2, activation='relu'))\n",
        "model_cce.add(Dense(2, activation='softmax'))\n",
        "model_cce.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model_cce.fit(X, y_categorical, epochs=100, verbose=0)\n",
        "print(\"Categorical Crossentropy:\", model_cce.evaluate(X, y_categorical, verbose=0))\n",
        "\n",
        "# Mean Squared Error (MSE)\n",
        "model_mse = Sequential()\n",
        "model_mse.add(Dense(4, input_dim=2, activation='relu'))\n",
        "model_mse.add(Dense(1))\n",
        "model_mse.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
        "model_mse.fit(X, y_binary, epochs=100, verbose=0)\n",
        "print(\"Mean Squared Error:\", model_mse.evaluate(X, y_binary, verbose=0))\n",
        "\n",
        "# Hinge Loss\n",
        "y_hinge = np.array([-1, 1, 1, -1])  # for hinge loss\n",
        "model_hinge = Sequential()\n",
        "model_hinge.add(Dense(4, input_dim=2, activation='tanh'))\n",
        "model_hinge.add(Dense(1, activation='tanh'))\n",
        "model_hinge.compile(optimizer='adam', loss='hinge', metrics=['accuracy'])\n",
        "model_hinge.fit(X, y_hinge, epochs=100, verbose=0)\n",
        "print(\"Hinge Loss:\", model_hinge.evaluate(X, y_hinge, verbose=0))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GSLzwUc871KP",
        "outputId": "f23e43d8-59df-4b8a-cf52-735d881265f1"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Binary Crossentropy: [0.6771039366722107, 0.75]\n",
            "Categorical Crossentropy: [0.682645320892334, 0.75]\n",
            "Mean Squared Error: [0.24641171097755432, 0.4603637456893921]\n",
            "Hinge Loss: [0.9844048023223877, 0.0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Backpropagation in Deep Learning"
      ],
      "metadata": {
        "id": "W44n9zmA7_lG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "# Sample XOR data\n",
        "X = np.array([[0,0], [0,1], [1,0], [1,1]])\n",
        "y = np.array([[0], [1], [1], [0]])\n",
        "\n",
        "# Define a simple neural network\n",
        "model = Sequential([\n",
        "    Dense(4, input_dim=2, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Compile the model with binary crossentropy loss and Adam optimizer\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model (backpropagation happens here)\n",
        "history = model.fit(X, y, epochs=200, verbose=0)\n",
        "\n",
        "# Final evaluation\n",
        "loss, acc = model.evaluate(X, y, verbose=0)\n",
        "print(f\"Final Loss: {loss:.4f}, Accuracy: {acc:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xrsw0W8z8Arw",
        "outputId": "3b8592a1-5362-4aa7-da0d-2997ec470598"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Loss: 0.6273, Accuracy: 0.7500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MLP Memoization\n",
        "\n",
        "✅ Example: Memoizing Predictions in MLP Using Python Dictionary\n",
        "\n"
      ],
      "metadata": {
        "id": "n8B1848b8I_T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "# Dataset\n",
        "X = np.array([[0,0], [0,1], [1,0], [1,1]])\n",
        "y = np.array([[0], [1], [1], [0]])\n",
        "\n",
        "# Define and train the MLP\n",
        "model = Sequential([\n",
        "    Dense(8, input_dim=2, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "model.fit(X, y, epochs=100, verbose=0)\n",
        "\n",
        "# Prediction Memoization\n",
        "prediction_cache = {}\n",
        "\n",
        "def predict_with_cache(input_data):\n",
        "    key = tuple(input_data)\n",
        "    if key in prediction_cache:\n",
        "        print(\"From Cache\")\n",
        "        return prediction_cache[key]\n",
        "    else:\n",
        "        prediction = model.predict(np.array([input_data]), verbose=0)[0][0]\n",
        "        prediction_cache[key] = prediction\n",
        "        print(\"From Model\")\n",
        "        return prediction\n",
        "\n",
        "# Test predictions with memoization\n",
        "print(predict_with_cache([0, 1]))\n",
        "print(predict_with_cache([0, 1]))  # This should use the cache\n",
        "print(predict_with_cache([1, 1]))\n",
        "print(predict_with_cache([1, 1]))  # Cached again\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0rztrhlM8J5K",
        "outputId": "efcab446-8af6-45bb-cec2-4aa728dba5ca"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:6 out of the last 6 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7a61e368a980> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "From Model\n",
            "0.65647143\n",
            "From Cache\n",
            "0.65647143\n",
            "From Model\n",
            "0.5284958\n",
            "From Cache\n",
            "0.5284958\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "# XOR Data\n",
        "X = np.array([[0,0], [0,1], [1,0], [1,1]])\n",
        "y = np.array([[0], [1], [1], [0]])\n",
        "\n",
        "\n",
        "\n",
        "model = Sequential([\n",
        "    Dense(8, input_dim=2, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eeeu5g5N8g3D",
        "outputId": "7cb41d15-61f2-4d4a-b29e-8f9969000c84"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "🔹 1. Batch Gradient Descent"
      ],
      "metadata": {
        "id": "NcrDrubU81xD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_batch = model\n",
        "model_batch.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=0.1),\n",
        "                    loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# batch_size = full dataset (4)\n",
        "model_batch.fit(X, y, epochs=200, batch_size=4, verbose=0)\n",
        "print(\"Batch Gradient Descent Evaluation:\")\n",
        "print(model_batch.evaluate(X, y, verbose=0))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G4DxfU-i8gsA",
        "outputId": "926a58a6-4c07-4f0a-833f-b163c925bd83"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Gradient Descent Evaluation:\n",
            "[0.2229798138141632, 1.0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "🔹 2. Stochastic Gradient Descent (SGD)"
      ],
      "metadata": {
        "id": "lhuEBd0G84RH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_sgd = model\n",
        "model_sgd.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=0.1),\n",
        "                  loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# batch_size = 1 (pure stochastic)\n",
        "model_sgd.fit(X, y, epochs=200, batch_size=1, verbose=0)\n",
        "print(\"Stochastic Gradient Descent Evaluation:\")\n",
        "print(model_sgd.evaluate(X, y, verbose=0))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-2HK3rjt8geM",
        "outputId": "26b103af-7ba7-46e7-8f85-76b2a2b9bbe5"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stochastic Gradient Descent Evaluation:\n",
            "[0.023348242044448853, 1.0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "🔹 3. Mini-Batch Gradient Descent"
      ],
      "metadata": {
        "id": "XxlnLGEN9AIv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_minibatch = model\n",
        "model_minibatch.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=0.1),\n",
        "                        loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# batch_size = 2 (mini-batch)\n",
        "model_minibatch.fit(X, y, epochs=200, batch_size=2, verbose=0)\n",
        "print(\"Mini-Batch Gradient Descent Evaluation:\")\n",
        "print(model_minibatch.evaluate(X, y, verbose=0))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aNdzQcni9BAZ",
        "outputId": "33f90efa-7695-481c-e368-3621c793941e"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mini-Batch Gradient Descent Evaluation:\n",
            "[0.015155991539359093, 1.0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Vanishing Gradient Problem in ANN | Exploding Gradient Problem | Code Example"
      ],
      "metadata": {
        "id": "hEKlXF9S9Jq6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "import numpy as np\n",
        "\n",
        "# Dummy data\n",
        "X = np.random.rand(1000, 10)\n",
        "y = np.random.randint(0, 2, size=(1000, 1))\n",
        "\n",
        "# Deep network\n",
        "def build_model(weight_initializer):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(128, input_shape=(10,), activation='relu', kernel_initializer=weight_initializer))\n",
        "    for _ in range(10):  # Very deep network to simulate gradient problems\n",
        "        model.add(Dense(128, activation='relu', kernel_initializer=weight_initializer))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy')\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "-vPiKo929XH_"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "🔹 1. Vanishing Gradient Problem"
      ],
      "metadata": {
        "id": "T2jNww7A9Z48"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Training with glorot_uniform (may vanish gradients)\")\n",
        "model_vanish = build_model('glorot_uniform')\n",
        "model_vanish.fit(X, y, epochs=10, verbose=1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7b31xXcL9aSh",
        "outputId": "4478de9c-de1b-45ff-b70b-c8963fa16b1a"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training with glorot_uniform (may vanish gradients)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.6926\n",
            "Epoch 2/10\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.6921\n",
            "Epoch 3/10\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.6901\n",
            "Epoch 4/10\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.6912\n",
            "Epoch 5/10\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.6883\n",
            "Epoch 6/10\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.6912\n",
            "Epoch 7/10\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.6854\n",
            "Epoch 8/10\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.6808\n",
            "Epoch 9/10\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.6808\n",
            "Epoch 10/10\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.6742\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7a61d09db590>"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "🔹 2. Exploding Gradient Problem"
      ],
      "metadata": {
        "id": "9BjeqXeR9e_K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.initializers import RandomNormal\n",
        "\n",
        "print(\"Training with random_normal stddev=5 (may explode gradients)\")\n",
        "exploding_init = RandomNormal(mean=0.0, stddev=5.0)\n",
        "model_explode = build_model(exploding_init)\n",
        "model_explode.fit(X, y, epochs=10, verbose=1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2W49c-5P9eQf",
        "outputId": "70fc3b0f-4a64-4247-a456-b06f99d9da9a"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training with random_normal stddev=5 (may explode gradients)\n",
            "Epoch 1/10\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - loss: 23084049056661504.0000\n",
            "Epoch 2/10\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 3379453140926464.0000\n",
            "Epoch 3/10\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1984876723044352.0000\n",
            "Epoch 4/10\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1461749974827008.0000\n",
            "Epoch 5/10\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1356124783640576.0000\n",
            "Epoch 6/10\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 967768539136000.0000\n",
            "Epoch 7/10\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 822194582061056.0000\n",
            "Epoch 8/10\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 710900638023680.0000\n",
            "Epoch 9/10\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 754263500259328.0000\n",
            "Epoch 10/10\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 680780032376832.0000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7a61d32643d0>"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Early Stopping In Neural Networks"
      ],
      "metadata": {
        "id": "x-_Zxvag9ymg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "# Sample data\n",
        "X = np.random.rand(1000, 10)\n",
        "y = np.random.randint(0, 2, size=(1000, 1))\n",
        "\n",
        "# Define model\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(64, activation='relu', input_shape=(10,)),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Compile\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# EarlyStopping callback\n",
        "early_stop = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor='val_loss',       # what to monitor\n",
        "    patience=3,               # how many epochs to wait\n",
        "    restore_best_weights=True\n",
        ")\n",
        "\n",
        "# Train with EarlyStopping\n",
        "model.fit(X, y,\n",
        "          validation_split=0.2,\n",
        "          epochs=100,\n",
        "          batch_size=32,\n",
        "          callbacks=[early_stop],\n",
        "          verbose=1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_k5IyWtg907l",
        "outputId": "5a88f8ca-d995-49e2-a52c-33330cf8e761"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.4861 - loss: 0.6987 - val_accuracy: 0.5050 - val_loss: 0.6969\n",
            "Epoch 2/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5143 - loss: 0.6929 - val_accuracy: 0.4900 - val_loss: 0.6966\n",
            "Epoch 3/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5258 - loss: 0.6908 - val_accuracy: 0.5000 - val_loss: 0.6935\n",
            "Epoch 4/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5208 - loss: 0.6915 - val_accuracy: 0.4700 - val_loss: 0.6951\n",
            "Epoch 5/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5277 - loss: 0.6872 - val_accuracy: 0.4800 - val_loss: 0.6967\n",
            "Epoch 6/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5499 - loss: 0.6849 - val_accuracy: 0.4850 - val_loss: 0.6947\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7a61d79c05d0>"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ✅ Feature Scaling in Neural Networks (StandardScaler)"
      ],
      "metadata": {
        "id": "yCBejIP3-BnP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "# Generate dummy data\n",
        "X = np.random.rand(1000, 10) * 100  # unscaled features\n",
        "y = np.random.randint(0, 2, size=(1000, 1))  # binary classification\n",
        "\n",
        "# Apply StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Define ANN model\n",
        "model = Sequential([\n",
        "    Dense(64, activation='relu', input_shape=(10,)),\n",
        "    Dense(32, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Compile model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train model\n",
        "model.fit(X_scaled, y, epochs=20, batch_size=32, validation_split=0.2)\n",
        "\n",
        "\n",
        "# 🔁 You can also try MinMaxScaler() if your activation is sigmoid or data range matters."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ozVHZIKd-CXy",
        "outputId": "b739b730-70a5-41bb-84a3-c61a2b357597"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.5221 - loss: 0.6957 - val_accuracy: 0.4700 - val_loss: 0.7082\n",
            "Epoch 2/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5420 - loss: 0.6922 - val_accuracy: 0.5000 - val_loss: 0.7044\n",
            "Epoch 3/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5953 - loss: 0.6763 - val_accuracy: 0.5300 - val_loss: 0.7021\n",
            "Epoch 4/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5794 - loss: 0.6706 - val_accuracy: 0.5300 - val_loss: 0.7018\n",
            "Epoch 5/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6333 - loss: 0.6659 - val_accuracy: 0.4900 - val_loss: 0.7049\n",
            "Epoch 6/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6093 - loss: 0.6670 - val_accuracy: 0.5050 - val_loss: 0.7030\n",
            "Epoch 7/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6227 - loss: 0.6574 - val_accuracy: 0.5050 - val_loss: 0.7071\n",
            "Epoch 8/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.6479 - loss: 0.6503 - val_accuracy: 0.4950 - val_loss: 0.7094\n",
            "Epoch 9/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6396 - loss: 0.6493 - val_accuracy: 0.4900 - val_loss: 0.7100\n",
            "Epoch 10/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6313 - loss: 0.6506 - val_accuracy: 0.4800 - val_loss: 0.7115\n",
            "Epoch 11/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6437 - loss: 0.6360 - val_accuracy: 0.4850 - val_loss: 0.7155\n",
            "Epoch 12/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6420 - loss: 0.6406 - val_accuracy: 0.4950 - val_loss: 0.7192\n",
            "Epoch 13/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6669 - loss: 0.6314 - val_accuracy: 0.4750 - val_loss: 0.7229\n",
            "Epoch 14/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.6690 - loss: 0.6349 - val_accuracy: 0.4850 - val_loss: 0.7253\n",
            "Epoch 15/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6935 - loss: 0.6187 - val_accuracy: 0.4700 - val_loss: 0.7268\n",
            "Epoch 16/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6923 - loss: 0.6135 - val_accuracy: 0.4750 - val_loss: 0.7322\n",
            "Epoch 17/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.6993 - loss: 0.6125 - val_accuracy: 0.4650 - val_loss: 0.7358\n",
            "Epoch 18/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7108 - loss: 0.6006 - val_accuracy: 0.4600 - val_loss: 0.7369\n",
            "Epoch 19/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.6798 - loss: 0.6189 - val_accuracy: 0.4550 - val_loss: 0.7424\n",
            "Epoch 20/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7010 - loss: 0.6073 - val_accuracy: 0.4650 - val_loss: 0.7417\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7a61dd418b10>"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dropout Layers in ANN |  Regression | Classification"
      ],
      "metadata": {
        "id": "8AXB2V5Y-Ovc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "✅ Classification Example with Dropout"
      ],
      "metadata": {
        "id": "RC1Sc10N-TE0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "import numpy as np\n",
        "\n",
        "# Sample classification data\n",
        "X = np.random.rand(1000, 20)\n",
        "y = np.random.randint(0, 2, size=(1000, 1))\n",
        "\n",
        "# Model with Dropout\n",
        "model = Sequential([\n",
        "    Dense(64, activation='relu', input_shape=(20,)),\n",
        "    Dropout(0.5),\n",
        "    Dense(32, activation='relu'),\n",
        "    Dropout(0.3),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "model.fit(X, y, epochs=20, batch_size=32, validation_split=0.2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_8dUW8TP-UlL",
        "outputId": "dca76559-8fe5-4c34-c4a6-0f328844ab73"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step - accuracy: 0.4803 - loss: 0.7155 - val_accuracy: 0.5000 - val_loss: 0.6917\n",
            "Epoch 2/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5066 - loss: 0.6969 - val_accuracy: 0.4850 - val_loss: 0.6919\n",
            "Epoch 3/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5153 - loss: 0.6962 - val_accuracy: 0.5300 - val_loss: 0.6920\n",
            "Epoch 4/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5371 - loss: 0.6901 - val_accuracy: 0.5150 - val_loss: 0.6921\n",
            "Epoch 5/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5571 - loss: 0.6898 - val_accuracy: 0.5150 - val_loss: 0.6915\n",
            "Epoch 6/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5254 - loss: 0.6917 - val_accuracy: 0.5050 - val_loss: 0.6911\n",
            "Epoch 7/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5779 - loss: 0.6859 - val_accuracy: 0.5100 - val_loss: 0.6908\n",
            "Epoch 8/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5051 - loss: 0.6944 - val_accuracy: 0.5000 - val_loss: 0.6910\n",
            "Epoch 9/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5234 - loss: 0.6865 - val_accuracy: 0.5000 - val_loss: 0.6905\n",
            "Epoch 10/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5154 - loss: 0.6891 - val_accuracy: 0.5200 - val_loss: 0.6906\n",
            "Epoch 11/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5269 - loss: 0.6919 - val_accuracy: 0.5350 - val_loss: 0.6908\n",
            "Epoch 12/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5495 - loss: 0.6853 - val_accuracy: 0.5400 - val_loss: 0.6911\n",
            "Epoch 13/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5336 - loss: 0.6880 - val_accuracy: 0.5150 - val_loss: 0.6911\n",
            "Epoch 14/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5207 - loss: 0.6884 - val_accuracy: 0.5200 - val_loss: 0.6900\n",
            "Epoch 15/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5460 - loss: 0.6880 - val_accuracy: 0.5250 - val_loss: 0.6901\n",
            "Epoch 16/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5107 - loss: 0.6860 - val_accuracy: 0.5350 - val_loss: 0.6904\n",
            "Epoch 17/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5731 - loss: 0.6823 - val_accuracy: 0.5200 - val_loss: 0.6908\n",
            "Epoch 18/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5631 - loss: 0.6876 - val_accuracy: 0.5100 - val_loss: 0.6907\n",
            "Epoch 19/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5363 - loss: 0.6865 - val_accuracy: 0.5150 - val_loss: 0.6912\n",
            "Epoch 20/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5317 - loss: 0.6907 - val_accuracy: 0.5400 - val_loss: 0.6907\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7a61dd25a3d0>"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "✅ Regression Example with Dropout"
      ],
      "metadata": {
        "id": "LoqN_n04-Wp8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Sample regression data\n",
        "X = np.random.rand(1000, 10) * 100\n",
        "y = np.random.rand(1000, 1) * 500\n",
        "\n",
        "# Scale inputs\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Model for regression\n",
        "model = Sequential([\n",
        "    Dense(64, activation='relu', input_shape=(10,)),\n",
        "    Dropout(0.4),\n",
        "    Dense(32, activation='relu'),\n",
        "    Dropout(0.2),\n",
        "    Dense(1)  # No activation for regression\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
        "model.fit(X_scaled, y, epochs=20, batch_size=32, validation_split=0.2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GECU6UqG-YM9",
        "outputId": "6c8c1dae-3ab5-4323-bbd1-23c346e5c69e"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - loss: 88015.8750 - mae: 261.0348 - val_loss: 87998.3516 - val_mae: 258.5094\n",
            "Epoch 2/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 85255.6484 - mae: 253.7667 - val_loss: 86812.2031 - val_mae: 256.1976\n",
            "Epoch 3/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 82464.1094 - mae: 246.8874 - val_loss: 84791.1016 - val_mae: 252.2059\n",
            "Epoch 4/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 78601.5469 - mae: 240.8858 - val_loss: 81522.9375 - val_mae: 245.8898\n",
            "Epoch 5/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 77642.2656 - mae: 239.0134 - val_loss: 76658.5547 - val_mae: 236.5915\n",
            "Epoch 6/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 72846.0078 - mae: 227.9276 - val_loss: 70066.0938 - val_mae: 224.0484\n",
            "Epoch 7/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 68801.7812 - mae: 222.8472 - val_loss: 61865.7266 - val_mae: 208.2841\n",
            "Epoch 8/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 57316.6133 - mae: 198.4286 - val_loss: 52797.6016 - val_mae: 190.8518\n",
            "Epoch 9/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 50547.1602 - mae: 186.1673 - val_loss: 43418.5586 - val_mae: 172.9837\n",
            "Epoch 10/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 41138.3984 - mae: 167.2721 - val_loss: 35396.3516 - val_mae: 156.8818\n",
            "Epoch 11/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 36131.2617 - mae: 158.8048 - val_loss: 29360.1406 - val_mae: 144.0046\n",
            "Epoch 12/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 27683.3965 - mae: 138.5433 - val_loss: 25669.0645 - val_mae: 136.0162\n",
            "Epoch 13/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 24356.8477 - mae: 132.2176 - val_loss: 23520.0625 - val_mae: 131.4825\n",
            "Epoch 14/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 23952.8516 - mae: 131.3643 - val_loss: 22560.5391 - val_mae: 129.0994\n",
            "Epoch 15/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 24140.9238 - mae: 131.4456 - val_loss: 22091.7070 - val_mae: 127.9984\n",
            "Epoch 16/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 23474.3984 - mae: 131.3919 - val_loss: 21916.9570 - val_mae: 127.5427\n",
            "Epoch 17/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 23033.2168 - mae: 127.9929 - val_loss: 21794.6309 - val_mae: 127.2371\n",
            "Epoch 18/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 23460.5059 - mae: 129.6423 - val_loss: 21853.3203 - val_mae: 127.5024\n",
            "Epoch 19/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 24031.7051 - mae: 132.4198 - val_loss: 21804.4824 - val_mae: 127.3223\n",
            "Epoch 20/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 22251.8574 - mae: 123.9913 - val_loss: 21776.7578 - val_mae: 127.2623\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7a61dd0a0050>"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Regularization in Deep Learning | L2 Regularization in ANN | L1 Regularization | Weight Decay in ANN\n"
      ],
      "metadata": {
        "id": "5aLG5Dww-jYY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "✅ L2 Regularization (Weight Decay)"
      ],
      "metadata": {
        "id": "9-ElfFBd-pAQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.regularizers import l2\n",
        "import numpy as np\n",
        "\n",
        "# Sample data\n",
        "X = np.random.rand(1000, 20)\n",
        "y = np.random.randint(0, 2, size=(1000, 1))\n",
        "\n",
        "# Model with L2 regularization\n",
        "model = Sequential([\n",
        "    Dense(64, activation='relu', kernel_regularizer=l2(0.01), input_shape=(20,)),\n",
        "    Dense(32, activation='relu', kernel_regularizer=l2(0.01)),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "model.fit(X, y, epochs=20, batch_size=32, validation_split=0.2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aSFy4toi-mKd",
        "outputId": "30ecb976-6bc5-419d-d481-3c2327f39efa"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 14ms/step - accuracy: 0.4996 - loss: 1.4093 - val_accuracy: 0.4900 - val_loss: 1.2865\n",
            "Epoch 2/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5116 - loss: 1.2500 - val_accuracy: 0.4500 - val_loss: 1.1593\n",
            "Epoch 3/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5463 - loss: 1.1278 - val_accuracy: 0.5100 - val_loss: 1.0558\n",
            "Epoch 4/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5697 - loss: 1.0268 - val_accuracy: 0.4600 - val_loss: 0.9764\n",
            "Epoch 5/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5760 - loss: 0.9523 - val_accuracy: 0.4950 - val_loss: 0.9127\n",
            "Epoch 6/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.5738 - loss: 0.8925 - val_accuracy: 0.5050 - val_loss: 0.8642\n",
            "Epoch 7/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.5567 - loss: 0.8474 - val_accuracy: 0.4900 - val_loss: 0.8287\n",
            "Epoch 8/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5710 - loss: 0.8132 - val_accuracy: 0.5100 - val_loss: 0.7979\n",
            "Epoch 9/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5730 - loss: 0.7849 - val_accuracy: 0.5000 - val_loss: 0.7773\n",
            "Epoch 10/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.5919 - loss: 0.7631 - val_accuracy: 0.5000 - val_loss: 0.7609\n",
            "Epoch 11/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.5788 - loss: 0.7491 - val_accuracy: 0.5100 - val_loss: 0.7465\n",
            "Epoch 12/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5823 - loss: 0.7373 - val_accuracy: 0.5250 - val_loss: 0.7368\n",
            "Epoch 13/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.5736 - loss: 0.7268 - val_accuracy: 0.4900 - val_loss: 0.7289\n",
            "Epoch 14/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5806 - loss: 0.7208 - val_accuracy: 0.5050 - val_loss: 0.7227\n",
            "Epoch 15/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5867 - loss: 0.7121 - val_accuracy: 0.4950 - val_loss: 0.7184\n",
            "Epoch 16/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5640 - loss: 0.7101 - val_accuracy: 0.5100 - val_loss: 0.7156\n",
            "Epoch 17/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5724 - loss: 0.7079 - val_accuracy: 0.5150 - val_loss: 0.7100\n",
            "Epoch 18/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5582 - loss: 0.7040 - val_accuracy: 0.5000 - val_loss: 0.7089\n",
            "Epoch 19/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5842 - loss: 0.6991 - val_accuracy: 0.5000 - val_loss: 0.7079\n",
            "Epoch 20/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5540 - loss: 0.6992 - val_accuracy: 0.5050 - val_loss: 0.7065\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7a61dcefde50>"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "✅ L1 Regularization"
      ],
      "metadata": {
        "id": "2VuJbQyT-snq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.regularizers import l1\n",
        "\n",
        "model = Sequential([\n",
        "    Dense(64, activation='relu', kernel_regularizer=l1(0.01), input_shape=(20,)),\n",
        "    Dense(32, activation='relu', kernel_regularizer=l1(0.01)),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n"
      ],
      "metadata": {
        "id": "9o9CtAZ9-tFN"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "✅ L1 + L2 Regularization (Elastic Net)"
      ],
      "metadata": {
        "id": "xS_Oo0_g-xQ7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.regularizers import l1_l2\n",
        "\n",
        "model = Sequential([\n",
        "    Dense(64, activation='relu', kernel_regularizer=l1_l2(l1=0.005, l2=0.005), input_shape=(20,)),\n",
        "    Dense(32, activation='relu', kernel_regularizer=l1_l2(l1=0.005, l2=0.005)),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n"
      ],
      "metadata": {
        "id": "WmWMYoaf-xxe"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Activation Functions in Deep Learning | Sigmoid, Tanh and Relu and Relu Variants Explained | Leaky Relu | Parametric Relu | Elu | Selu"
      ],
      "metadata": {
        "id": "tEhtegln-8Ow"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, LeakyReLU, PReLU, ELU\n",
        "model = Sequential([\n",
        "    # Input Layer\n",
        "    Dense(64, activation='sigmoid', input_shape=(20,)),   # Sigmoid Activation\n",
        "\n",
        "    # Hidden Layers with different activations\n",
        "    Dense(64, activation='tanh'),                          # Tanh Activation\n",
        "    Dense(64, activation='relu'),                          # ReLU Activation\n",
        "\n",
        "    Dense(64), LeakyReLU(alpha=0.1),                       # Leaky ReLU\n",
        "    Dense(64), PReLU(),                                    # Parametric ReLU (learns alpha)\n",
        "    Dense(64), ELU(alpha=1.0),                             # ELU\n",
        "    Dense(64, activation='selu'),                          # SELU (only useful with specific initializers)\n",
        "\n",
        "    # Output Layer (Binary Classification)\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "X = np.random.rand(1000, 20)\n",
        "y = np.random.randint(0, 2, (1000, 1))\n",
        "\n",
        "model.fit(X, y, epochs=10, batch_size=32, validation_split=0.2)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TsGutuIG_EBe",
        "outputId": "40ed89f3-67ca-4d09-819b-0cda26d810eb"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 27ms/step - accuracy: 0.5249 - loss: 0.6950 - val_accuracy: 0.5250 - val_loss: 0.6930\n",
            "Epoch 2/10\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.5159 - loss: 0.6949 - val_accuracy: 0.4800 - val_loss: 0.6956\n",
            "Epoch 3/10\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.5549 - loss: 0.6904 - val_accuracy: 0.4800 - val_loss: 0.6987\n",
            "Epoch 4/10\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5106 - loss: 0.6976 - val_accuracy: 0.4700 - val_loss: 0.6932\n",
            "Epoch 5/10\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4901 - loss: 0.6946 - val_accuracy: 0.4800 - val_loss: 0.6946\n",
            "Epoch 6/10\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.4903 - loss: 0.6941 - val_accuracy: 0.4800 - val_loss: 0.6959\n",
            "Epoch 7/10\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4850 - loss: 0.6953 - val_accuracy: 0.4800 - val_loss: 0.6944\n",
            "Epoch 8/10\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5004 - loss: 0.6943 - val_accuracy: 0.4800 - val_loss: 0.6945\n",
            "Epoch 9/10\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5070 - loss: 0.6933 - val_accuracy: 0.4800 - val_loss: 0.6949\n",
            "Epoch 10/10\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5122 - loss: 0.6934 - val_accuracy: 0.4800 - val_loss: 0.6946\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7a61d6f06290>"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Weight Initialization Techniques Xavier/Glorat And He Weight Initialization\n"
      ],
      "metadata": {
        "id": "NTrjCIny_XWW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.initializers import RandomNormal, GlorotUniform, HeNormal\n",
        "import numpy as np\n",
        "\n"
      ],
      "metadata": {
        "id": "7LOxAkuz_bBW"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "✅ 2. Xavier/Glorot Initialization (Good for tanh/sigmoid activations)"
      ],
      "metadata": {
        "id": "Q8WoRkLW_j2z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = np.random.rand(500, 100)\n",
        "y = np.random.randint(0, 2, (500, 1))\n",
        "model_xavier = Sequential([\n",
        "    Dense(64, input_shape=(100,), kernel_initializer=GlorotUniform(), activation='tanh'),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model_he.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "model_he.fit(X, y, epochs=5, batch_size=32)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "83U0yDaz_l4w",
        "outputId": "882fe377-a0c7-4698-cadf-3716138c9b3a"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.4586 - loss: 0.7292\n",
            "Epoch 2/5\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5574 - loss: 0.7007 \n",
            "Epoch 3/5\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5537 - loss: 0.6918\n",
            "Epoch 4/5\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5925 - loss: 0.6749  \n",
            "Epoch 5/5\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5320 - loss: 0.6858 \n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7a61d7481e50>"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "✅ 3. He Initialization (Best for ReLU/LeakyReLU activations)"
      ],
      "metadata": {
        "id": "M5KEgAbF_nhQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = np.random.rand(500, 100)\n",
        "y = np.random.randint(0, 2, (500, 1))\n",
        "model_he = Sequential([\n",
        "    Dense(64, input_shape=(100,), kernel_initializer=HeNormal(), activation='relu'),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "model_he.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "model_he.fit(X, y, epochs=5, batch_size=32)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VmbGmde3_o5j",
        "outputId": "4ed198b3-3ff4-4110-8840-18e4f976fd3d"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.5363 - loss: 0.7086\n",
            "Epoch 2/5\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5124 - loss: 0.7072 \n",
            "Epoch 3/5\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4794 - loss: 0.7065 \n",
            "Epoch 4/5\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5693 - loss: 0.6927 \n",
            "Epoch 5/5\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5919 - loss: 0.6741 \n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7a61d7953e90>"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Batch Normalization in Deep Learning | Batch Learning in Keras\n"
      ],
      "metadata": {
        "id": "oRzW8fCUADmC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "✅ Batch Normalization for Classification"
      ],
      "metadata": {
        "id": "ZwPSLog-AG-y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, BatchNormalization, Activation\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Sample model with Batch Normalization\n",
        "model = Sequential([\n",
        "    Dense(128, input_shape=(100,)),\n",
        "    BatchNormalization(),\n",
        "    Activation('relu'),\n",
        "\n",
        "    Dense(64),\n",
        "    BatchNormalization(),\n",
        "    Activation('relu'),\n",
        "\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Dummy Data\n",
        "import numpy as np\n",
        "X = np.random.rand(1000, 100)\n",
        "y = np.random.randint(0, 2, (1000, 1))\n",
        "\n",
        "model.fit(X, y, epochs=5, batch_size=32)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i_QRoxdGAE9t",
        "outputId": "6742e772-0afe-41fa-86e8-d269906670aa"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.4516 - loss: 0.8506\n",
            "Epoch 2/5\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6276 - loss: 0.6411\n",
            "Epoch 3/5\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7219 - loss: 0.5750\n",
            "Epoch 4/5\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8229 - loss: 0.5055\n",
            "Epoch 5/5\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8717 - loss: 0.4568\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7a61defd5550>"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "✅ Batch Normalization for Regression"
      ],
      "metadata": {
        "id": "kBhqQp85ALEF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_reg = Sequential([\n",
        "    Dense(128, input_shape=(100,)),\n",
        "    BatchNormalization(),\n",
        "    Activation('relu'),\n",
        "\n",
        "    Dense(64),\n",
        "    BatchNormalization(),\n",
        "    Activation('relu'),\n",
        "\n",
        "    Dense(1)  # No activation for regression output\n",
        "])\n",
        "\n",
        "model_reg.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "# Dummy Regression Data\n",
        "X = np.random.rand(1000, 100)\n",
        "y = np.random.rand(1000, 1)\n",
        "\n",
        "model_reg.fit(X, y, epochs=5, batch_size=32)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-njxiTonAMuB",
        "outputId": "f81b75f1-0c96-47e3-fb75-131d00d6a809"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 1.0010\n",
            "Epoch 2/5\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2249\n",
            "Epoch 3/5\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1536\n",
            "Epoch 4/5\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1033\n",
            "Epoch 5/5\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0735\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7a61e3ce1610>"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Optimizers in Deep Learning\n"
      ],
      "metadata": {
        "id": "yLnBxp3zAS4Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "# Dummy Data\n",
        "X = np.random.rand(1000, 20)\n",
        "y = np.random.randint(0, 2, (1000, 1))\n"
      ],
      "metadata": {
        "id": "X-jox9coARvS"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. SGD (Stochastic Gradient Descent)"
      ],
      "metadata": {
        "id": "Bxzu9z3QAcLm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_sgd = Sequential([\n",
        "    Dense(64, activation='relu', input_shape=(20,)),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model_sgd.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=0.01),\n",
        "                  loss='binary_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "model_sgd.fit(X, y, epochs=5, batch_size=32)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WtjLK52dAdJA",
        "outputId": "1730819e-c261-47f4-89e1-cd874ea814de"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.4886 - loss: 0.7000\n",
            "Epoch 2/5\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.4933 - loss: 0.6965\n",
            "Epoch 3/5\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.4862 - loss: 0.6952\n",
            "Epoch 4/5\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5465 - loss: 0.6891\n",
            "Epoch 5/5\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5155 - loss: 0.6973\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7a61e3934c50>"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Momentum"
      ],
      "metadata": {
        "id": "8fYtmHXXAgY1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_momentum = Sequential([\n",
        "    Dense(64, activation='relu', input_shape=(20,)),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model_momentum.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=0.01, momentum=0.9),\n",
        "                       loss='binary_crossentropy',\n",
        "                       metrics=['accuracy'])\n",
        "\n",
        "model_momentum.fit(X, y, epochs=5, batch_size=32)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QYGvsubrAg2-",
        "outputId": "c4ae31c1-c4ba-4ca8-ab32-d4aa8890e76e"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.4759 - loss: 0.7081\n",
            "Epoch 2/5\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5032 - loss: 0.7008\n",
            "Epoch 3/5\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5083 - loss: 0.6974\n",
            "Epoch 4/5\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5032 - loss: 0.6990\n",
            "Epoch 5/5\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5177 - loss: 0.6936\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7a61e3ac6c10>"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. RMSprop"
      ],
      "metadata": {
        "id": "d2ncHbyMAjz4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_rmsprop = Sequential([\n",
        "    Dense(64, activation='relu', input_shape=(20,)),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model_rmsprop.compile(optimizer=tf.keras.optimizers.RMSprop(learning_rate=0.001),\n",
        "                      loss='binary_crossentropy',\n",
        "                      metrics=['accuracy'])\n",
        "\n",
        "model_rmsprop.fit(X, y, epochs=5, batch_size=32)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l7ZagWtoAkR4",
        "outputId": "0a4b4e63-2a7b-4c29-e425-e33a0b0b55d4"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5000 - loss: 0.7040\n",
            "Epoch 2/5\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5116 - loss: 0.6938\n",
            "Epoch 3/5\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5361 - loss: 0.6898\n",
            "Epoch 4/5\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5491 - loss: 0.6875\n",
            "Epoch 5/5\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5148 - loss: 0.6885\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7a61e382d3d0>"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Adam"
      ],
      "metadata": {
        "id": "g91eNpiGAnIw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_adam = Sequential([\n",
        "    Dense(64, activation='relu', input_shape=(20,)),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model_adam.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
        "                   loss='binary_crossentropy',\n",
        "                   metrics=['accuracy'])\n",
        "\n",
        "model_adam.fit(X, y, epochs=5, batch_size=32)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KHbBSubDAoWW",
        "outputId": "430661d6-eb91-4d1f-d8ca-f9d0d3b4c38f"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.4914 - loss: 0.7053\n",
            "Epoch 2/5\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5116 - loss: 0.6930\n",
            "Epoch 3/5\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5101 - loss: 0.6949\n",
            "Epoch 4/5\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5465 - loss: 0.6877\n",
            "Epoch 5/5\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5498 - loss: 0.6885\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7a61d71f8dd0>"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exponentially Weighted Moving Average or Exponential Weighted Average"
      ],
      "metadata": {
        "id": "coM6FLfZAxw6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "import numpy as np\n",
        "\n",
        "# Dummy data\n",
        "X = np.random.rand(1000, 10)\n",
        "y = np.random.randint(0, 2, size=(1000, 1))\n",
        "\n",
        "# Model\n",
        "model = Sequential([\n",
        "    Dense(32, activation='relu', input_shape=(10,)),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Using SGD with momentum (which uses EWMA internally)\n",
        "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01, momentum=0.9)\n",
        "\n",
        "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "model.fit(X, y, epochs=5, batch_size=32)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ymTlSJPUA0Ii",
        "outputId": "db2e8399-7dfd-4972-cd6a-575fa3246fd2"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.4839 - loss: 0.7040\n",
            "Epoch 2/5\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4819 - loss: 0.7028\n",
            "Epoch 3/5\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5176 - loss: 0.6941\n",
            "Epoch 4/5\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5479 - loss: 0.6891\n",
            "Epoch 5/5\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5337 - loss: 0.6901\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7a61ca04f050>"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Nesterov Accelerated Gradient (NAG)"
      ],
      "metadata": {
        "id": "zsfY-4V1BM-g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "import numpy as np\n",
        "\n",
        "# Dummy dataset\n",
        "X = np.random.rand(1000, 10)\n",
        "y = np.random.randint(0, 2, size=(1000, 1))\n",
        "\n",
        "# Build model\n",
        "model = Sequential([\n",
        "    Dense(32, activation='relu', input_shape=(10,)),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Compile using SGD with Nesterov\n",
        "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01, momentum=0.9, nesterov=True)\n",
        "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train\n",
        "model.fit(X, y, epochs=5, batch_size=32)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O2bWzCpyBN_e",
        "outputId": "f363ab73-0b26-4b70-e302-45d383fc2507"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.4714 - loss: 0.7121\n",
            "Epoch 2/5\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5234 - loss: 0.6918\n",
            "Epoch 3/5\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5329 - loss: 0.6893\n",
            "Epoch 4/5\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.5476 - loss: 0.6886\n",
            "Epoch 5/5\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5659 - loss: 0.6832\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7a61c9fcd110>"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# AdaGrad"
      ],
      "metadata": {
        "id": "Hxv-zq7sBZ4a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "import numpy as np\n",
        "\n",
        "# Dummy dataset\n",
        "X = np.random.rand(1000, 10)\n",
        "y = np.random.randint(0, 2, size=(1000, 1))\n",
        "\n",
        "# Model\n",
        "model = Sequential([\n",
        "    Dense(64, activation='relu', input_shape=(10,)),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Compile using AdaGrad\n",
        "optimizer = tf.keras.optimizers.Adagrad(learning_rate=0.01)\n",
        "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train\n",
        "model.fit(X, y, epochs=5, batch_size=32)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cdbDcKioBau-",
        "outputId": "cec173f0-c651-47be-c627-393d25651121"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.5148 - loss: 0.6963\n",
            "Epoch 2/5\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5001 - loss: 0.6951\n",
            "Epoch 3/5\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5335 - loss: 0.6932\n",
            "Epoch 4/5\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5215 - loss: 0.6930\n",
            "Epoch 5/5\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5095 - loss: 0.6943\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7a61c9e40050>"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Keras Tuner | Hyperparameter Tuning a Neural Network"
      ],
      "metadata": {
        "id": "Y1VgCpKrBp58"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Keras Tuner with Hyperband"
      ],
      "metadata": {
        "id": "zOAedLf5Buhr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keras-tuner --upgrade\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hrt_ysFXBzOQ",
        "outputId": "17a342e9-ddb5-4e7b-bef8-173e3aeafc36"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting keras-tuner\n",
            "  Downloading keras_tuner-1.4.7-py3-none-any.whl.metadata (5.4 kB)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.11/dist-packages (from keras-tuner) (3.8.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from keras-tuner) (25.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from keras-tuner) (2.32.3)\n",
            "Collecting kt-legacy (from keras-tuner)\n",
            "  Downloading kt_legacy-1.0.5-py3-none-any.whl.metadata (221 bytes)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from keras->keras-tuner) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from keras->keras-tuner) (2.0.2)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras->keras-tuner) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras->keras-tuner) (0.1.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.11/dist-packages (from keras->keras-tuner) (3.14.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras->keras-tuner) (0.17.0)\n",
            "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.11/dist-packages (from keras->keras-tuner) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->keras-tuner) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->keras-tuner) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->keras-tuner) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->keras-tuner) (2025.7.14)\n",
            "Requirement already satisfied: typing-extensions>=4.6.0 in /usr/local/lib/python3.11/dist-packages (from optree->keras->keras-tuner) (4.14.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras->keras-tuner) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras->keras-tuner) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras->keras-tuner) (0.1.2)\n",
            "Downloading keras_tuner-1.4.7-py3-none-any.whl (129 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.1/129.1 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading kt_legacy-1.0.5-py3-none-any.whl (9.6 kB)\n",
            "Installing collected packages: kt-legacy, keras-tuner\n",
            "Successfully installed keras-tuner-1.4.7 kt-legacy-1.0.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import keras_tuner as kt\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "\n",
        "# Dummy Data\n",
        "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "X_train, X_test = X_train / 255.0, X_test / 255.0\n",
        "\n",
        "# Flatten input\n",
        "X_train = X_train.reshape(-1, 28 * 28)\n",
        "X_test = X_test.reshape(-1, 28 * 28)\n",
        "\n",
        "# Build Model Function\n",
        "def build_model(hp):\n",
        "    model = keras.Sequential()\n",
        "    model.add(keras.layers.Input(shape=(784,)))\n",
        "\n",
        "    # Tune number of layers\n",
        "    for i in range(hp.Int('num_layers', 1, 3)):\n",
        "        model.add(keras.layers.Dense(\n",
        "            units=hp.Int(f'units_{i}', min_value=32, max_value=512, step=32),\n",
        "            activation=hp.Choice('activation', ['relu', 'tanh'])\n",
        "        ))\n",
        "\n",
        "    model.add(keras.layers.Dense(10, activation='softmax'))\n",
        "\n",
        "    # Compile\n",
        "    model.compile(\n",
        "        optimizer=keras.optimizers.Adam(\n",
        "            hp.Choice('learning_rate', [1e-2, 1e-3, 1e-4])\n",
        "        ),\n",
        "        loss='sparse_categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    return model\n",
        "\n",
        "# Initialize Tuner\n",
        "tuner = kt.Hyperband(\n",
        "    build_model,\n",
        "    objective='val_accuracy',\n",
        "    max_epochs=10,\n",
        "    factor=3,\n",
        "    directory='keras_tuner_dir',\n",
        "    project_name='mnist_tuning'\n",
        ")\n",
        "\n",
        "# Early stopping\n",
        "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n",
        "\n",
        "# Search best hyperparameters\n",
        "tuner.search(X_train, y_train, epochs=20, validation_split=0.2, callbacks=[stop_early])\n",
        "\n",
        "# Get best model\n",
        "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "\n",
        "print(f\"Best hyperparameters:\")\n",
        "print(f\" - Layers: {best_hps.get('num_layers')}\")\n",
        "print(f\" - Activation: {best_hps.get('activation')}\")\n",
        "print(f\" - Learning rate: {best_hps.get('learning_rate')}\")\n",
        "\n",
        "# Train best model\n",
        "model = tuner.hypermodel.build(best_hps)\n",
        "model.fit(X_train, y_train, epochs=1, validation_split=0.2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 738
        },
        "id": "Id7tcRP2Bu7t",
        "outputId": "49b00a2f-6333-46f2-f0d4-296ba61b2e41"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 7 Complete [00h 00m 48s]\n",
            "val_accuracy: 0.9595833420753479\n",
            "\n",
            "Best val_accuracy So Far: 0.9698333144187927\n",
            "Total elapsed time: 00h 04m 38s\n",
            "\n",
            "Search: Running Trial #8\n",
            "\n",
            "Value             |Best Value So Far |Hyperparameter\n",
            "1                 |2                 |num_layers\n",
            "160               |192               |units_0\n",
            "relu              |relu              |activation\n",
            "0.01              |0.001             |learning_rate\n",
            "480               |32                |units_1\n",
            "288               |None              |units_2\n",
            "2                 |2                 |tuner/epochs\n",
            "0                 |0                 |tuner/initial_epoch\n",
            "2                 |2                 |tuner/bracket\n",
            "0                 |0                 |tuner/round\n",
            "\n",
            "Epoch 1/2\n",
            "\u001b[1m 574/1500\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.8430 - loss: 0.5056"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-746141240.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;31m# Search best hyperparameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m \u001b[0mtuner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstop_early\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;31m# Get best model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras_tuner/src/engine/base_tuner.py\u001b[0m in \u001b[0;36msearch\u001b[0;34m(self, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_trial_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_run_and_update_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mfit_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_trial_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_search_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras_tuner/src/engine/base_tuner.py\u001b[0m in \u001b[0;36m_try_run_and_update_trial\u001b[0;34m(self, trial, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[1;32m    272\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_try_run_and_update_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mfit_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 274\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_and_update_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mfit_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    275\u001b[0m             \u001b[0mtrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrial_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrialStatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOMPLETED\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras_tuner/src/engine/base_tuner.py\u001b[0m in \u001b[0;36m_run_and_update_trial\u001b[0;34m(self, trial, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_and_update_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mfit_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 239\u001b[0;31m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mfit_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    240\u001b[0m         if self.oracle.get_trial(trial.trial_id).metrics.exists(\n\u001b[1;32m    241\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moracle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobjective\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras_tuner/src/tuners/hyperband.py\u001b[0m in \u001b[0;36mrun_trial\u001b[0;34m(self, trial, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[1;32m    425\u001b[0m             \u001b[0mfit_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"epochs\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"tuner/epochs\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m             \u001b[0mfit_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"initial_epoch\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"tuner/initial_epoch\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 427\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mfit_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    428\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_build_hypermodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras_tuner/src/engine/tuner.py\u001b[0m in \u001b[0;36mrun_trial\u001b[0;34m(self, trial, *args, **kwargs)\u001b[0m\n\u001b[1;32m    312\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_checkpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m             \u001b[0mcopied_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"callbacks\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 314\u001b[0;31m             \u001b[0mobj_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_and_fit_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcopied_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m             \u001b[0mhistories\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras_tuner/src/engine/tuner.py\u001b[0m in \u001b[0;36m_build_and_fit_model\u001b[0;34m(self, trial, *args, **kwargs)\u001b[0m\n\u001b[1;32m    231\u001b[0m         \u001b[0mhp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhyperparameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_build\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 233\u001b[0;31m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhypermodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m         \u001b[0;31m# Save the build config for model loading later.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras_tuner/src/engine/hypermodel.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, hp, model, *args, **kwargs)\u001b[0m\n\u001b[1;32m    147\u001b[0m             \u001b[0mIf\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0ma\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mit\u001b[0m \u001b[0mshould\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mthe\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mobjective\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \"\"\"\n\u001b[0;32m--> 149\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    369\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mepoch_iterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 371\u001b[0;31m                     \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    372\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mfunction\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m    217\u001b[0m                 \u001b[0miterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDistributedIterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m             ):\n\u001b[0;32m--> 219\u001b[0;31m                 \u001b[0mopt_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmulti_step_on_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    220\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mopt_outputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    831\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 833\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    876\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    877\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 878\u001b[0;31m       results = tracing_compilation.call_function(\n\u001b[0m\u001b[1;32m    879\u001b[0m           \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    880\u001b[0m       )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m   \u001b[0mbound_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m   \u001b[0mflat_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbound_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m   return function._call_flat(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    140\u001b[0m       \u001b[0mflat_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m   )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1320\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1321\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1324\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;34m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0mflat_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_recording\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bound_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m             outputs = self._bound_context.call_function(\n\u001b[0m\u001b[1;32m    252\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1681\u001b[0m     \u001b[0mcancellation_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcancellation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1682\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcancellation_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1683\u001b[0;31m       outputs = execute.execute(\n\u001b[0m\u001b[1;32m   1684\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1685\u001b[0m           \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}